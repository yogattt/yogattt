---
title: >-
  Exploiting intra- and inter-session dependencies for session-based
  recommendations
date: 2022-05-01 13:28:19
tags:
  - 2022
  - WWW
  - session-based
categories: 推荐系统
mathjax: true
---
## Abstract

&emsp;&emsp;目前大多数回话推荐通常仅根据回话内的依赖关系做预测，忽略了复杂的会话间关系和其它可获得的边信息（item属性，users）,限制了推荐系统的准确性。为了从会话信息和边信息中有效提取会话间和会话内的依赖，进一步提升下一个item预测的准确性，作者提出了一个新的超图学习框架。该框架包含三个模块：  

- 超图构造模块（hypergraph construction module），构造一个超图，以统一的方式将用户、项目和项目属性连接在一起。  
- 超图学习模块（hypergraph learning module），通过提取构建超图中嵌入的会话内和会话间依赖项，学习每个 item 和 user 的信息潜在表示。
- 下一个item预测模块（next-item predection module)。讲学习到的表示送入预测模块用于推荐。


## Introduction

&emsp;&emsp;two challenges :  

- ch1: 如何有效地充分利用可用的附带信息，包括项目属性信息和用户信息，以提高下一个项目建议的准确性？
- ch2: 如何有效地提取会话内和会话间的依赖关系，以进一步改进下一个项目的推荐？

&emsp;&emsp;针对ch1,作者同时引入item属性信息和user信息，以丰富下一个项目推荐的信息。为了有效地整合此类边信息，作者构建了一个超图，通过将基于会话的item-tiem图、item-item attribute value 图和user-session图集成到统一的超图中，将项目、项目属性值和用户连接在一起。  
&emsp;&emsp;针对ch2，对于会话内的依赖关系，作者将构建在每个会话上的每个子图转换为门控图神经网络，以了解会话内项目之间的依赖关系。对于会话间的依赖关系，作者设计了三种类型的聚合操作(user aggregation,item aggregation,attribute aggregation)，以了解会话间的依赖性，以吸收其他会话的信息，从而有利于当前会话的下一个item推荐。每个聚合都基于超图中的一种超边。  
&emsp;&emsp;除此之外，作者还使用了session agregation学习用户的偏好。具体来说，session aggregation聚合用户的所有历史会话，以代表她/他的偏好.

![](figure1.png)

<center> 图1：基于user-item信息和item-attribute信息构建的超图示例。</center>

## Problem statement

$$
\begin{array}{|c|c|}
\hline
{符号}&{含义} \\
\hline
{\mathcal{U}=\left\{u_{1}, u_{2}, \ldots, u_{|u|}\right\}} & {包含所有不同user的user集合} \\
\hline 
{\mathcal{V}^{I}=\left\{v_{1}, v_{2}, \ldots, v_{\left|\mathcal{V}^{I}\right|}\right\}}&{包含所有不同item的item集合} \\
\hline 
{\mathcal{D}=\left\{S_{1}, S_{2}, \ldots, S_{|u|}\right\}}&{数据集中所有的用户事物集合} \\
\hline 
{S_{i}=\left\{s_{i, 1}, s_{i, 2}, \ldots, s_{i,|s|}\right\}}&{与用户i相关的session集合} \\
\hline 
{s_{i, j}=\left\{v_{i, j, 1}, v_{i, j, 2, \ldots}\right.\left.v_{i, j,\left|s_{i, j}\right|}\right\}}&{用户i的第j个session} \\
\hline 
{\mathcal{V}^{\mathcal{A}}=\left\{a_{1}, a_{2}, \ldots, a_{\left|\mathcal{V}^{A}\right|}\right\}}&{数据集中所有item的属性值集合} \\
\hline 
{\mathcal{A}=\left\{A_{1}, A_{2}, \ldots, A_{\left|\mathcal{V}^{I}\right|}\right\}}&{所有item的属性集合} \\
\hline 
{A_{h}=\left\{a_{h, 1}, a_{h, 2}, \ldots, a_{h,\left|A_{h}\right|}\right\}}&{item \ v_h\ 的属性值集合} \\
\hline 
{C_{v_{l}}^{s_{i}}=\left\{v_{1}, v_{2}, \ldots, v_{l-1}\right\}}&{对应会话的上下文，顺序包含session\ s_{i,j} 中目标item\ v_j\ 之前出现的所有item} \\
\hline 
{C_{v_{l}}^{a}=\left\{\mathcal{A}_{v_{1}}, \mathcal{A}_{v_{2}}, \ldots, \mathcal{A}_{v_{l-1}}\right\}}&{对应的属性上下文} \\
\hline
\end{array} 
$$

## Hypergraph learning framework

### **Preliminaries**
&emsp;&emsp;图表只能表示成对关系。相比之下，超图保留了多方面关系，因此是建模复杂关系的自然选择。将超图定义如下：

![](figure2.png)

<center> 图2：超图学习框架工作流程 </center>

在有限顶点集$\mathcal{V}=\left\{v_{i}: i \in\|n\|\right\}$上带有超边$\mathcal{E}=\left\{e_{j}: j \in\|p\|\right\}$的超图$\mathcal{G}=\{\mathcal{V}, \mathcal{E}\}$,每个超边都是 $\mathcal{V}$ 的非空子集，满足$\cup_{j \in\|p\|} \| e_{j}=\mathcal{V}$

### **Hypergraph construction**
&emsp;&emsp;首先在图中建立users,items，items attribute values三种类型的节点。因此图中的节点集合为$\mathcal{V}=\mathcal{U} \cup \mathcal{V}^{\mathcal{I}} \cup \mathcal{V}^{\mathcal{A}}$,然后在超图中建立user-session relations, the item-item relations, and the item-attribute value relations 三种超边关系。因此超图中超边集为$\mathcal{E}=\mathcal{E}^{\mathcal{U}} \cup \mathcal{E}^{\mathcal{I}} \cup \mathcal{E}^{\mathcal{A}}$。

- 超边$e_{\mathcal{u}}\in \mathcal{E}^{\mathcal{U}}$ 表示用户$u$购买的session $s_{u}=\left\{v_{u, 1}, v_{u, 2}, \ldots, v_{u,\left|s_{u}\right|}\right\}$,换句话说，用户u在一个会话中购买的项目$v_{u, 1}, v_{u, 2}, \ldots, v_{u, \mid s_{u} \mid} $通过这个超边$e_u$链接。
- 超边$e_{i,j}^I\in \mathcal{E}^{\mathcal{I}}$ 表示在某个会话中 item $v_j$出现在item $v_i$ 后面。
- 超边$e_i^A\in \mathcal{E}^{\mathcal{A}}$ 表示item $v_i$带有属性值集合 $A_{i}=\left\{a_{i, 1}, a_{i, 2}, \ldots, a_{i,\left|v_{i}\right|}\right\}$,item $v_i$通过超边与这些属性值相连。

### **Hypergraph learning**

&emsp;&emsp;HL主要由三个部分组成（1）node level aggregation (2) node level updating and (3) graph level learning. 

#### **Node level aggregation in hypergraph**

&emsp;&emsp;首先将user node $u\in \mathcal{U}$ 和 item node $v\in\mathcal{V}^{I}$ 映射到低维隐空间获得初始的表示 $\mathbf{u}, \mathbf{v} \in \mathbb{R}^{d}$。然后，对于每一个item $v\in\mathcal{V}^{I}$,通过在 $\mathcal G $中分别从三种类型的 $v$ 的邻域节点中吸收信息，使用三种聚合操作得到三种类型的隐向量表达 $\mathbf{v}_{i}^{U}, \mathbf{v}_{i}^{I}, \mathbf{v}_{i}^{A}$。从特定角度来看，每种类型的潜在表示都可以被视为item的子表示。对于每一个user $u\in\mathcal{U}$，通过学习他的历史session和当前session中的偏好来更新隐表示。  
&emsp;&emsp;**user aggregation** 用于学习item $v_i$ 的 user-based latent representation $\mathbf{v}_{i}^{U}\in \mathbb{R}^{d}$。用户聚合的目的是汇总来自联合用户的项目的信息。
$$ \mathbf{v}_{i}^{U}=A g g r e_{u s e r}\left\{\mathbf{v}_{j}, \forall v_{j} \in N_{u s e r}\left(v_{i}\right)\right\} $$
$N_{u s e r}\left(v_{i}\right)$是item $v_i$ 基于用户的领域集，包含的item $v_j$ 满足与item $v_i$ 有同一 user 交互。指定用户聚合如下:
$${Aggre}_{user}\left(\mathbf{v}_{i}\right)=\mathbf{A}_{i}^{u}\left[\mathbf{v}_{i, 1}, \mathbf{v}_{i, 2}, \ldots, \mathbf{v}_{i,\left|N_{u s e r}\left(v_{i}\right)\right|}\right]^{T}$$

权重矩阵 $\mathbf{A}_{i}^u$ 表示 $N_{u s e r}\left(v_{i}\right)$ 中每个item 对于 $v_i$ 的重要性。$\mathbf{A}_{i}^u$ 计算方式如下：
$$\mathbf{A}_{i, j}^{u}=\frac{\left\|\mathcal{V}_{i, j}^{e_{u}}\right\|_{2}}{\left\|\mathcal{V}_{i}^{e_{u}}\right\|_{2}+\left\|\mathcal{V}_{j}^{e_{u}}\right\|_{2}}$$
其中，$\mathcal{V}_{i, j}^{e_{u}}=\left\{u \mid\left(v_{i}, v_{j}, u\right) \in e_{u}\right\}$ 表示与 $v_i\ v_j$ 都相连的用户的数量。$\mathcal{V}_{i}^{e_{u}}=\left\{u \mid\left(v_{i}, u\right) \in e_{u}\right\}$ 表示与$v_i$相连的用户的数量。  
&emsp;&emsp;**item aggregation**  用于学习item $v_i$ 的 inter-session-based latent representation $\mathbf{v}_{i}^{I}\in \mathbb{R}^{d}$。item聚合的目的是汇总与item $v_i$ 在同一session上出现的其他item的信息。
$$\mathbf{v}_{i}^{I}=A g g r e_{\text {sess }}\left\{\mathbf{v}_{j}, \forall v_{j} \in N_{sess}\left(v_{i}\right)\right\}$$
$N_{sess}$ 是 item $v_i$ 基于session的领域集，它包含超图中与item $v_i$ 在同一会话中出现的所有 item 节点。指定 item 聚合如下：
$${Aggre} e_{i t e m s}\left(\mathbf{v}_{i}\right)=\mathbf{A}_{i}^{s}\left[\mathbf{v}_{i, 1}, \mathbf{v}_{i, 2}, \ldots, \mathbf{v}_{i,|{sess}|}\right]^{T}$$
$\left[\mathbf{v}_{i, 1}, \mathbf{v}_{i, 2}, \ldots, \mathbf{v}_{i,|{sess}|}\right]$ 是item nodes ${v}_{i, 1}, {v}_{i, 2}, \ldots, {v}_{i,|{sess}|} \in N_{sess}$ 的隐表示列表。权重矩阵 $\mathbf{A}_{i}^s$ 表示 $N_{sess}\left(v_{i}\right)$ 中每个item 对于 $v_i$ 的重要性。$\mathbf{A}_{i}^s$ 计算方式如下：
$$\mathbf{A}_{i, j}^{s}=\frac{\left\|\mathcal{V}_{i, j}^{e^I}\right\|_{2}}{\left\|\mathcal{V}_{i}^{e^I}\right\|_{2}+\left\|\mathcal{V}_{j}^{e^I}\right\|_{2}}$$
其中，$\mathcal{V}_{i, j}^{e^I}=\{sess|(v_i,v_j,sess)\in e^I\}$ 表示同时包含 $v_i,v_j$ 的session 的数量。$\mathcal{V}_{i}^{e^I}=\{sess|(v_i,sess)\in e^I\}$ 表示包含 $v_i$ 的 session 的数量。  
&emsp;&emsp;**attribute aggregation** 用于学习item $v_i$ 的 attribute-based latent representation $\mathbf{v}_{i}^{A}\in \mathbb{R}^{d}$。属性聚合的目的是聚合与item $v_i$ 有共享属性值的 item 的信息。
$$\mathbf{v}_{i}^{A}=A g g r e_{\text {attri }}\left\{\mathbf{v}_{j}, \forall v_{j} \in N_{attri}\left(v_{i}\right)\right\}$$
$${Aggre} e_{attri}\left(\mathbf{v}_{i}\right)=\mathbf{A}_{i}^{A}\left[\mathbf{v}_{i, 1}, \mathbf{v}_{i, 2}, \ldots, \mathbf{v}_{i,|{attri}|}\right]^{T}$$
$$\mathbf{A}_{i, j}^{A}=\frac{\left\|\mathcal{V}_{i, j}^{e^A}\right\|_{2}}{\left\|\mathcal{V}_{i}^{e^A}\right\|_{2}+\left\|\mathcal{V}_{j}^{e^A}\right\|_{2}}$$  

#### **Node level updating in hypergraph**  

item $v_i$ 的最终隐表示：
$$\mathbf{v}_i=\sigma\left(\mathbf{W} \cdot Concat\left[\mathbf{v}_{i}^{U}, \mathbf{v}_{i}^{I}, \mathbf{v}_{i}^{A} \right]+b \right)$$
然后，对于每一个 item $v_i$,通过给定 session 中其它节点信息，使用门控图神经网络迭代更新 $v_i$ 的表示 $\mathbf{v}_i$。因此 intra-session 依赖信息被学习并编码为 item 表示。首先根据连接矩阵 $\mathbf{M}^s$ 从 item $v_i$ 的intra-session-based 邻居item 得到聚合信息 $\texttt{a}_{s,j}^t$

$$\texttt{a}_{s,j}^t=\mathbf{M}_{i,:}^s[\mathbf{v}_{s,1}^{t-1},\mathbf{v}_{s,2}^{t-1},\cdots,\mathbf{v}_{s,|s|}^{t-1}]^{T}\mathbf{H}+\mathbf{b}$$

其中 $\mathbf{H}\in \mathbb{R}^{d\times d}$ 是权重矩阵,$\mathbb{b}$ 是偏置。$[\mathbf{v}_{s,1}^{t-1},\mathbf{v}_{s,2}^{t-1},\cdots,\mathbf{v}_{s,|s|}^{t-1}]$ 是第（t-1）次迭代时，节点 $v_{s,i}$ 的intra-session-based 邻居节点 $v_{i,1},v_{i,2},\cdots,v_{i,|s|}$ 的隐表示列表。连接矩阵 $\mathbf{M}^s=[\mathbf{M}^{s,I},\mathbf{M}^{s,O}]\in \mathbb{R}^{|s|\times 2|s|}$ 为 $\mathbf{M}^{s,I},\mathbf{M}^{s,O}$ 的拼接。$\mathbf{M}_{i,:}^s$ 表示 $\mathbf{M}^s$ 的第i行。矩阵 $\mathbf{M}^{s,I},\mathbf{M}^{s,O}$ 表示session s 中 两个 item 节点的连接性，计算方式如下:
$$\mathbf{M}_{i,j}^{s,I}=\frac{|e_{i,j}^I|}{|D^I(v_i)|}\ \ \ \ \ \ \mathbf{M}_{i,j}^{s,O}=\frac{|e_{i,j}^O|}{|D^O(v_i)|}$$
其中,$e_{i,j}^I$ 表示从 $v_i$ 连接到 $v_j$ 的超边集合。$D^I(v_i),D^O(v_i)$分别表示 $v_i$ 的入度和出度。

![](figure3.png)

$$  图3：基于会话s和相应连接矩阵M_{i}^s的有向子图示例 $$
&emsp;&emsp;迭代更新过程如下:
$$\mathbf{z}_{s,i}^t=\sigma(\mathbf{W}_{\mathcal{z}}\texttt{a}_{s,i}^t+\mathbf{U}_{\mathcal{z}}\mathbf{v}_{s,i}^{t-1})$$
$$\mathbf{r}_{s,i}^t=\sigma(\mathbf{W}_{\mathcal{r}}\texttt{a}_{s,i}^t+\mathbf{U}_{\mathcal{r}}\mathbf{v}_{s,i}^{t-1})$$
$$\tilde{\mathbf{v}}_{s,i}^t=tanh(\mathbf{W}_{\mathcal{o}}\texttt{a}_{s,i}^t+\mathbf{U}_{\mathcal{o}}(\mathbf{r}_{s,i}^t\odot\mathbf{v}_{s,i}^{t-1}))$$
$$\mathbf{v}_{s,i}^t=(1-\mathbf{z}_{s,i}^t)\odot\mathbf{v}_{s,i}^{t-1}+\mathbf{z}_{s,i}^t\odot\tilde{\mathbf{v}}_{s,i}^t$$
$\mathbf{r}_{s,i}^t$ 是重置门矢量，$\tilde{\mathbf{v}}_{s,i}^t$ 是更新门矢量。$\mathbf{W}_{\mathcal{z}}$,$\mathbf{W}_{\mathcal{r}}$,$\mathbf{W}_{\mathcal{o}}\in\mathbb{R}^{2d\times d}$ 和 $\mathbf{U}_{\mathcal{z}}$,$\mathbf{U}_{\mathcal{r}}$,$\mathbf{U}_{\mathcal{o}}$是可学习权重矩阵。$\sigma$ 为sigmoid 激活函数。$\odot$ 为元素乘法。  
&emsp;&emsp;**session aggregation** 聚合user $u_i$ 的历史和当前session 信息。
$$Aggre^u(\mathbf u_i)=\mathbf{A}_{i}^{U}\left[\mathbf{v}_{i, 1}, \mathbf{v}_{i, 2}, \ldots, \mathbf{v}_{i,|{N_{u_i}}|}\right]^{T}$$
$N_{u_i}$ 为user $u_i$ 历史和当前session 中的所有 item 节点集合。$\mathbf{A}_{i}^{U}$ 计算方式如下：
$$\mathbf{A}_{i, j}^{U}=\frac{\left\|\mathcal{V}_{i, j}^{e_{u}}\right\|_{2}}{\left\|\mathcal{V}_{i, j}\right\|_{2}}$$
$\mathcal{V}_{i, j}^{e_{u}}=\{v_j|(v_i,v_j,u)\in e_u\}$ 表示与用户 $u$ 和item $v_i$ 相连的 item 的数量。node $u_i$ 的最终表达为
$$ \mathbf{u}_i^{Final}=\sigma(\mathbf{W}\cdot [Aggre^u(\mathbf{u}_i),\mathbf{u}_i])$$  

#### **Graph learning in hypergraph**  

&emsp;&emsp;给定一个user $u_i$ 的会话上下文 $C=\{v_{c,1},v_{c,2},\cdots,v_{c,|C|}\}$,其表示 $\mathbf{C}$ 计算方式如下：
$$\mathbf{C}_0=\mathbf{u}^{Final}$$
$$\mathbf{C}_t^*=LSTM(\mathbf{C}_{t-1})$$
将user $\mathbf{u}$ 的隐表示作为初始向量 $\mathbf{C}_0$,然后使用LSTM迭代更新它。$\mathbf{C}_t^*\in \mathbb{R}^d$ 表示 query vector，用于计算对于item node $v_{c,i}$ 的attention vector $e_{c,i,t}$。
$$e_{c,i,t}=f(\mathbf{v}_{c,i},\mathbf{C}_t^*)$$
$$a_{c,i,t}=\frac{exp(e_{c,i,t})}{\sum_{c,j}exp(e_{c,j,t})}$$
&emsp;&emsp;item node $v_{c,i}$ 的 attention vector $e_{c,i,t}$ 由 query vector $\mathbf{C}_t^*$ 和 item node 的隐表示 $\mathbf v_{c,i}$ 共同决定，$f$ 是attention function。然后计算注意力概率得分 $a_{c,i,t}$
$$r_t=\sum_{c,i}a_{c,i,t}\mathbf{v}_i$$
$$\mathbf{C}_t=Contact[\mathbf{C}_t^*,r_t]$$
最后输出 $\mathbf{C}_t$ 作为user session contex 隐表示用于下一item推荐。  

### **Prediction and optimization**  

$$\hat y_i=softmax((\mathbf v_i)^T\mathbf{C})$$
$$Loss=-\sum_{i=1}^my_ilog(\hat y_i)+(1-y_i)log(1-\hat y_i)$$  

## Experiment setup  


<center> 表1：实验数据集统计表 </center>

![](table1.png)


<center> 表2：实验结果对比表 </center>

![](table2.png)



