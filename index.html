<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hello World">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hello World">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="yogattt">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hello World</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hello World</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/01/Exploiting-intra-and-inter-session-dependencies-for-session-based-recommendations/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="yogattt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/01/Exploiting-intra-and-inter-session-dependencies-for-session-based-recommendations/" class="post-title-link" itemprop="url">Exploiting intra- and inter-session dependencies for session-based recommendations</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-05-01 13:28:19 / Modified: 22:39:23" itemprop="dateCreated datePublished" datetime="2022-05-01T13:28:19+08:00">2022-05-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">推荐系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="abstract">Abstract</h2>
<p>  目前大多数回话推荐通常仅根据回话内的依赖关系做预测，忽略了复杂的会话间关系和其它可获得的边信息（item属性，users）,限制了推荐系统的准确性。为了从会话信息和边信息中有效提取会话间和会话内的依赖，进一步提升下一个item预测的准确性，作者提出了一个新的超图学习框架。该框架包含三个模块：
- 超图构造模块（hypergraph construction
module），构造一个超图，以统一的方式将用户、项目和项目属性连接在一起。<br />
- 超图学习模块（hypergraph learning
module），通过提取构建超图中嵌入的会话内和会话间依赖项，学习每个 item 和
user 的信息潜在表示。 - 下一个item预测模块（next-item predection
module)。讲学习到的表示送入预测模块用于推荐。</p>
<h2 id="introduction">Introduction</h2>
<p>  two challenges :</p>
<ul>
<li>ch1:
如何有效地充分利用可用的附带信息，包括项目属性信息和用户信息，以提高下一个项目建议的准确性？</li>
<li>ch2:
如何有效地提取会话内和会话间的依赖关系，以进一步改进下一个项目的推荐？</li>
</ul>
<p>  针对ch1,作者同时引入item属性信息和user信息，以丰富下一个项目推荐的信息。为了有效地整合此类边信息，作者构建了一个超图，通过将基于会话的item-tiem图、item-item
attribute value
图和user-session图集成到统一的超图中，将项目、项目属性值和用户连接在一起。<br />
  针对ch2，对于会话内的依赖关系，作者将构建在每个会话上的每个子图转换为门控图神经网络，以了解会话内项目之间的依赖关系。对于会话间的依赖关系，作者设计了三种类型的聚合操作(user
aggregation,item aggregation,attribute
aggregation)，以了解会话间的依赖性，以吸收其他会话的信息，从而有利于当前会话的下一个item推荐。每个聚合都基于超图中的一种超边。<br />
  除此之外，作者还使用了session
agregation学习用户的偏好。具体来说，session
aggregation聚合用户的所有历史会话，以代表她/他的偏好.</p>
<p><img src="figure1.png" /></p>
<center>
图1：基于user-item信息和item-attribute信息构建的超图示例。
</center>
<h2 id="problem-statement">Problem statement</h2>
<p><span class="math display">\[
\begin{array}{|c|c|}
\hline
{符号}&amp;{含义} \\
\hline
{\mathcal{U}=\left\{u_{1}, u_{2}, \ldots, u_{|u|}\right\}} &amp;
{包含所有不同user的user集合} \\
\hline
{\mathcal{V}^{I}=\left\{v_{1}, v_{2}, \ldots,
v_{\left|\mathcal{V}^{I}\right|}\right\}}&amp;{包含所有不同item的item集合}
\\
\hline
{\mathcal{D}=\left\{S_{1}, S_{2}, \ldots,
S_{|u|}\right\}}&amp;{数据集中所有的用户事物集合} \\
\hline
{S_{i}=\left\{s_{i, 1}, s_{i, 2}, \ldots,
s_{i,|s|}\right\}}&amp;{与用户i相关的session集合} \\
\hline
{s_{i, j}=\left\{v_{i, j, 1}, v_{i, j, 2, \ldots}\right.\left.v_{i,
j,\left|s_{i, j}\right|}\right\}}&amp;{用户i的第j个session} \\
\hline
{\mathcal{V}^{\mathcal{A}}=\left\{a_{1}, a_{2}, \ldots,
a_{\left|\mathcal{V}^{A}\right|}\right\}}&amp;{数据集中所有item的属性值集合}
\\
\hline
{\mathcal{A}=\left\{A_{1}, A_{2}, \ldots,
A_{\left|\mathcal{V}^{I}\right|}\right\}}&amp;{所有item的属性集合} \\
\hline
{A_{h}=\left\{a_{h, 1}, a_{h, 2}, \ldots,
a_{h,\left|A_{h}\right|}\right\}}&amp;{item \ v_h\ 的属性值集合} \\
\hline
{C_{v_{l}}^{s_{i}}=\left\{v_{1}, v_{2}, \ldots,
v_{l-1}\right\}}&amp;{对应会话的上下文，顺序包含session\ s_{i,j}
中目标item\ v_j\ 之前出现的所有item} \\
\hline
{C_{v_{l}}^{a}=\left\{\mathcal{A}_{v_{1}}, \mathcal{A}_{v_{2}}, \ldots,
\mathcal{A}_{v_{l-1}}\right\}}&amp;{对应的属性上下文} \\
\hline
\end{array}
\]</span></p>
<h2 id="hypergraph-learning-framework">Hypergraph learning
framework</h2>
<h3 id="preliminaries"><strong>Preliminaries</strong></h3>
<p>  图表只能表示成对关系。相比之下，超图保留了多方面关系，因此是建模复杂关系的自然选择。将超图定义如下：</p>
<p><img src="figure2.png" /></p>
<center>
图2：超图学习框架工作流程
</center>
<p>在有限顶点集<span class="math inline">\(\mathcal{V}=\left\{v_{i}: i
\in\|n\|\right\}\)</span>上带有超边<span
class="math inline">\(\mathcal{E}=\left\{e_{j}: j
\in\|p\|\right\}\)</span>的超图<span
class="math inline">\(\mathcal{G}=\{\mathcal{V},
\mathcal{E}\}\)</span>,每个超边都是 <span
class="math inline">\(\mathcal{V}\)</span> 的非空子集，满足<span
class="math inline">\(\cup_{j \in\|p\|} \|
e_{j}=\mathcal{V}\)</span></p>
<h3 id="hypergraph-construction"><strong>Hypergraph
construction</strong></h3>
<p>  首先在图中建立users,items，items attribute
values三种类型的节点。因此图中的节点集合为<span
class="math inline">\(\mathcal{V}=\mathcal{U} \cup
\mathcal{V}^{\mathcal{I}} \cup
\mathcal{V}^{\mathcal{A}}\)</span>,然后在超图中建立user-session
relations, the item-item relations, and the item-attribute value
relations 三种超边关系。因此超图中超边集为<span
class="math inline">\(\mathcal{E}=\mathcal{E}^{\mathcal{U}} \cup
\mathcal{E}^{\mathcal{I}} \cup \mathcal{E}^{\mathcal{A}}\)</span>。</p>
<ul>
<li>超边<span class="math inline">\(e_{\mathcal{u}}\in
\mathcal{E}^{\mathcal{U}}\)</span> 表示用户<span
class="math inline">\(u\)</span>购买的session <span
class="math inline">\(s_{u}=\left\{v_{u, 1}, v_{u, 2}, \ldots,
v_{u,\left|s_{u}\right|}\right\}\)</span>,换句话说，用户u在一个会话中购买的项目$v_{u,
1}, v_{u, 2}, , v_{u, s_{u} } <span
class="math inline">\(通过这个超边\)</span>e_u$链接。</li>
<li>超边<span class="math inline">\(e_{i,j}^I\in
\mathcal{E}^{\mathcal{I}}\)</span> 表示在某个会话中 item <span
class="math inline">\(v_j\)</span>出现在item <span
class="math inline">\(v_i\)</span> 后面。</li>
<li>超边<span class="math inline">\(e_i^A\in
\mathcal{E}^{\mathcal{A}}\)</span> 表示item <span
class="math inline">\(v_i\)</span>带有属性值集合 <span
class="math inline">\(A_{i}=\left\{a_{i, 1}, a_{i, 2}, \ldots,
a_{i,\left|v_{i}\right|}\right\}\)</span>,item <span
class="math inline">\(v_i\)</span>通过超边与这些属性值相连。</li>
</ul>
<h3 id="hypergraph-learning"><strong>Hypergraph learning</strong></h3>
<p>  HL主要由三个部分组成（1）node level aggregation (2) node level
updating and (3) graph level learning.</p>
<h4 id="node-level-aggregation-in-hypergraph"><strong>Node level
aggregation in hypergraph</strong></h4>
<p>  首先将user node <span class="math inline">\(u\in
\mathcal{U}\)</span> 和 item node <span
class="math inline">\(v\in\mathcal{V}^{I}\)</span>
映射到低维隐空间获得初始的表示 <span class="math inline">\(\mathbf{u},
\mathbf{v} \in \mathbb{R}^{d}\)</span>。然后，对于每一个item <span
class="math inline">\(v\in\mathcal{V}^{I}\)</span>,通过在 $G
$中分别从三种类型的 <span class="math inline">\(v\)</span>
的邻域节点中吸收信息，使用三种聚合操作得到三种类型的隐向量表达 <span
class="math inline">\(\mathbf{v}_{i}^{U}, \mathbf{v}_{i}^{I},
\mathbf{v}_{i}^{A}\)</span>。从特定角度来看，每种类型的潜在表示都可以被视为item的子表示。对于每一个user
<span
class="math inline">\(u\in\mathcal{U}\)</span>，通过学习他的历史session和当前session中的偏好来更新隐表示。<br />
  <strong>user aggregation</strong> 用于学习item <span
class="math inline">\(v_i\)</span> 的 user-based latent representation
<span class="math inline">\(\mathbf{v}_{i}^{U}\in
\mathbb{R}^{d}\)</span>。用户聚合的目的是汇总来自联合用户的项目的信息。
<span class="math display">\[ \mathbf{v}_{i}^{U}=A g g r e_{u s e
r}\left\{\mathbf{v}_{j}, \forall v_{j} \in N_{u s e
r}\left(v_{i}\right)\right\} \]</span> <span class="math inline">\(N_{u
s e r}\left(v_{i}\right)\)</span>是item <span
class="math inline">\(v_i\)</span> 基于用户的领域集，包含的item <span
class="math inline">\(v_j\)</span> 满足与item <span
class="math inline">\(v_i\)</span> 有同一 user 交互。指定用户聚合如下:
<span
class="math display">\[{Aggre}_{user}\left(\mathbf{v}_{i}\right)=\mathbf{A}_{i}^{u}\left[\mathbf{v}_{i,
1}, \mathbf{v}_{i, 2}, \ldots, \mathbf{v}_{i,\left|N_{u s e
r}\left(v_{i}\right)\right|}\right]^{T}\]</span></p>
<p>权重矩阵 <span class="math inline">\(\mathbf{A}_{i}^u\)</span> 表示
<span class="math inline">\(N_{u s e r}\left(v_{i}\right)\)</span>
中每个item 对于 <span class="math inline">\(v_i\)</span> 的重要性。<span
class="math inline">\(\mathbf{A}_{i}^u\)</span> 计算方式如下： <span
class="math display">\[\mathbf{A}_{i,
j}^{u}=\frac{\left\|\mathcal{V}_{i,
j}^{e_{u}}\right\|_{2}}{\left\|\mathcal{V}_{i}^{e_{u}}\right\|_{2}+\left\|\mathcal{V}_{j}^{e_{u}}\right\|_{2}}\]</span>
其中，<span class="math inline">\(\mathcal{V}_{i, j}^{e_{u}}=\left\{u
\mid\left(v_{i}, v_{j}, u\right) \in e_{u}\right\}\)</span> 表示与 <span
class="math inline">\(v_i\ v_j\)</span> 都相连的用户的数量。<span
class="math inline">\(\mathcal{V}_{i}^{e_{u}}=\left\{u \mid\left(v_{i},
u\right) \in e_{u}\right\}\)</span> 表示与<span
class="math inline">\(v_i\)</span>相连的用户的数量。<br />
  <strong>item aggregation</strong> 用于学习item <span
class="math inline">\(v_i\)</span> 的 inter-session-based latent
representation <span class="math inline">\(\mathbf{v}_{i}^{I}\in
\mathbb{R}^{d}\)</span>。item聚合的目的是汇总与item <span
class="math inline">\(v_i\)</span> 在同一session上出现的其他item的信息。
<span class="math display">\[\mathbf{v}_{i}^{I}=A g g r e_{\text {sess
}}\left\{\mathbf{v}_{j}, \forall v_{j} \in
N_{sess}\left(v_{i}\right)\right\}\]</span> <span
class="math inline">\(N_{sess}\)</span> 是 item <span
class="math inline">\(v_i\)</span>
基于session的领域集，它包含超图中与item <span
class="math inline">\(v_i\)</span> 在同一会话中出现的所有 item
节点。指定 item 聚合如下： <span class="math display">\[{Aggre} e_{i t e
m s}\left(\mathbf{v}_{i}\right)=\mathbf{A}_{i}^{s}\left[\mathbf{v}_{i,
1}, \mathbf{v}_{i, 2}, \ldots,
\mathbf{v}_{i,|{sess}|}\right]^{T}\]</span> <span
class="math inline">\(\left[\mathbf{v}_{i, 1}, \mathbf{v}_{i, 2},
\ldots, \mathbf{v}_{i,|{sess}|}\right]\)</span> 是item nodes <span
class="math inline">\({v}_{i, 1}, {v}_{i, 2}, \ldots, {v}_{i,|{sess}|}
\in N_{sess}\)</span> 的隐表示列表。权重矩阵 <span
class="math inline">\(\mathbf{A}_{i}^s\)</span> 表示 <span
class="math inline">\(N_{sess}\left(v_{i}\right)\)</span> 中每个item
对于 <span class="math inline">\(v_i\)</span> 的重要性。<span
class="math inline">\(\mathbf{A}_{i}^s\)</span> 计算方式如下： <span
class="math display">\[\mathbf{A}_{i,
j}^{s}=\frac{\left\|\mathcal{V}_{i,
j}^{e^I}\right\|_{2}}{\left\|\mathcal{V}_{i}^{e^I}\right\|_{2}+\left\|\mathcal{V}_{j}^{e^I}\right\|_{2}}\]</span>
其中，<span class="math inline">\(\mathcal{V}_{i,
j}^{e^I}=\{sess|(v_i,v_j,sess)\in e^I\}\)</span> 表示同时包含 <span
class="math inline">\(v_i,v_j\)</span> 的session 的数量。<span
class="math inline">\(\mathcal{V}_{i}^{e^I}=\{sess|(v_i,sess)\in
e^I\}\)</span> 表示包含 <span class="math inline">\(v_i\)</span> 的
session 的数量。<br />
  <strong>attribute aggregation</strong> 用于学习item <span
class="math inline">\(v_i\)</span> 的 attribute-based latent
representation <span class="math inline">\(\mathbf{v}_{i}^{A}\in
\mathbb{R}^{d}\)</span>。属性聚合的目的是聚合与item <span
class="math inline">\(v_i\)</span> 有共享属性值的 item 的信息。 <span
class="math display">\[\mathbf{v}_{i}^{A}=A g g r e_{\text {attri
}}\left\{\mathbf{v}_{j}, \forall v_{j} \in
N_{attri}\left(v_{i}\right)\right\}\]</span> <span
class="math display">\[{Aggre}
e_{attri}\left(\mathbf{v}_{i}\right)=\mathbf{A}_{i}^{A}\left[\mathbf{v}_{i,
1}, \mathbf{v}_{i, 2}, \ldots,
\mathbf{v}_{i,|{attri}|}\right]^{T}\]</span> <span
class="math display">\[\mathbf{A}_{i,
j}^{A}=\frac{\left\|\mathcal{V}_{i,
j}^{e^A}\right\|_{2}}{\left\|\mathcal{V}_{i}^{e^A}\right\|_{2}+\left\|\mathcal{V}_{j}^{e^A}\right\|_{2}}\]</span></p>
<h3 id="node-level-updating-in-hypergraph"><strong>Node level updating
in hypergraph</strong></h3>
<p>item <span class="math inline">\(v_i\)</span> 的最终隐表示： <span
class="math display">\[\mathbf{v}_i=\sigma\left(\mathbf{W} \cdot
Concat\left[\mathbf{v}_{i}^{U}, \mathbf{v}_{i}^{I}, \mathbf{v}_{i}^{A}
\right]+b \right)\]</span> 然后，对于每一个 item <span
class="math inline">\(v_i\)</span>,通过给定 session
中其它节点信息，使用门控图神经网络迭代更新 <span
class="math inline">\(v_i\)</span> 的表示 <span
class="math inline">\(\mathbf{v}_i\)</span>。因此 intra-session
依赖信息被学习并编码为 item 表示。首先根据连接矩阵 <span
class="math inline">\(\mathbf{M}^s\)</span> 从 item <span
class="math inline">\(v_i\)</span> 的intra-session-based 邻居item
得到聚合信息 <span class="math inline">\(\texttt{a}_{s,j}^t\)</span></p>
<p><span
class="math display">\[\texttt{a}_{s,j}^t=\mathbf{M}_{i,:}^s[\mathbf{v}_{s,1}^{t-1},\mathbf{v}_{s,2}^{t-1},\cdots,\mathbf{v}_{s,|s|}^{t-1}]^{T}\mathbf{H}+\mathbf{b}\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{H}\in \mathbb{R}^{d\times
d}\)</span> 是权重矩阵,<span class="math inline">\(\mathbb{b}\)</span>
是偏置。<span
class="math inline">\([\mathbf{v}_{s,1}^{t-1},\mathbf{v}_{s,2}^{t-1},\cdots,\mathbf{v}_{s,|s|}^{t-1}]\)</span>
是第（t-1）次迭代时，节点 <span class="math inline">\(v_{s,i}\)</span>
的intra-session-based 邻居节点 <span
class="math inline">\(v_{i,1},v_{i,2},\cdots,v_{i,|s|}\)</span>
的隐表示列表。连接矩阵 <span
class="math inline">\(\mathbf{M}^s=[\mathbf{M}^{s,I},\mathbf{M}^{s,O}]\in
\mathbb{R}^{|s|\times 2|s|}\)</span> 为 <span
class="math inline">\(\mathbf{M}^{s,I},\mathbf{M}^{s,O}\)</span>
的拼接。<span class="math inline">\(\mathbf{M}_{i,:}^s\)</span> 表示
<span class="math inline">\(\mathbf{M}^s\)</span> 的第i行。矩阵 <span
class="math inline">\(\mathbf{M}^{s,I},\mathbf{M}^{s,O}\)</span>
表示session s 中 两个 item 节点的连接性，计算方式如下: <span
class="math display">\[\mathbf{M}_{i,j}^{s,I}=\frac{|e_{i,j}^I|}{|D^I(v_i)|}\
\ \ \ \ \ \mathbf{M}_{i,j}^{s,O}=\frac{|e_{i,j}^O|}{|D^O(v_i)|}\]</span>
其中,<span class="math inline">\(e_{i,j}^I\)</span> 表示从 <span
class="math inline">\(v_i\)</span> 连接到 <span
class="math inline">\(v_j\)</span> 的超边集合。<span
class="math inline">\(D^I(v_i),D^O(v_i)\)</span>分别表示 <span
class="math inline">\(v_i\)</span> 的入度和出度。</p>
<p><img src="figure3.png" /></p>
<p><span
class="math display">\[  图3：基于会话s和相应连接矩阵M_{i}^s的有向子图示例
\]</span>   迭代更新过程如下: <span
class="math display">\[\mathbf{z}_{s,i}^t=\sigma(\mathbf{W}_{\mathcal{z}}\texttt{a}_{s,i}^t+\mathbf{U}_{\mathcal{z}}\mathbf{v}_{s,i}^{t-1})\]</span>
<span
class="math display">\[\mathbf{r}_{s,i}^t=\sigma(\mathbf{W}_{\mathcal{r}}\texttt{a}_{s,i}^t+\mathbf{U}_{\mathcal{r}}\mathbf{v}_{s,i}^{t-1})\]</span>
<span
class="math display">\[\tilde{\mathbf{v}}_{s,i}^t=tanh(\mathbf{W}_{\mathcal{o}}\texttt{a}_{s,i}^t+\mathbf{U}_{\mathcal{o}}(\mathbf{r}_{s,i}^t\odot\mathbf{v}_{s,i}^{t-1}))\]</span>
<span
class="math display">\[\mathbf{v}_{s,i}^t=(1-\mathbf{z}_{s,i}^t)\odot\mathbf{v}_{s,i}^{t-1}+\mathbf{z}_{s,i}^t\odot\tilde{\mathbf{v}}_{s,i}^t\]</span>
<span class="math inline">\(\mathbf{r}_{s,i}^t\)</span>
是重置门矢量，<span
class="math inline">\(\tilde{\mathbf{v}}_{s,i}^t\)</span>
是更新门矢量。<span
class="math inline">\(\mathbf{W}_{\mathcal{z}}\)</span>,<span
class="math inline">\(\mathbf{W}_{\mathcal{r}}\)</span>,<span
class="math inline">\(\mathbf{W}_{\mathcal{o}}\in\mathbb{R}^{2d\times
d}\)</span> 和 <span
class="math inline">\(\mathbf{U}_{\mathcal{z}}\)</span>,<span
class="math inline">\(\mathbf{U}_{\mathcal{r}}\)</span>,<span
class="math inline">\(\mathbf{U}_{\mathcal{o}}\)</span>是可学习权重矩阵。<span
class="math inline">\(\sigma\)</span> 为sigmoid 激活函数。<span
class="math inline">\(\odot\)</span> 为元素乘法。<br />
  <strong>session aggregation</strong> 聚合user <span
class="math inline">\(u_i\)</span> 的历史和当前session 信息。 <span
class="math display">\[Aggre^u(\mathbf
u_i)=\mathbf{A}_{i}^{U}\left[\mathbf{v}_{i, 1}, \mathbf{v}_{i, 2},
\ldots, \mathbf{v}_{i,|{N_{u_i}}|}\right]^{T}\]</span> <span
class="math inline">\(N_{u_i}\)</span> 为user <span
class="math inline">\(u_i\)</span> 历史和当前session 中的所有 item
节点集合。<span class="math inline">\(\mathbf{A}_{i}^{U}\)</span>
计算方式如下： <span class="math display">\[\mathbf{A}_{i,
j}^{U}=\frac{\left\|\mathcal{V}_{i,
j}^{e_{u}}\right\|_{2}}{\left\|\mathcal{V}_{i, j}\right\|_{2}}\]</span>
<span class="math inline">\(\mathcal{V}_{i,
j}^{e_{u}}=\{v_j|(v_i,v_j,u)\in e_u\}\)</span> 表示与用户 <span
class="math inline">\(u\)</span> 和item <span
class="math inline">\(v_i\)</span> 相连的 item 的数量。node <span
class="math inline">\(u_i\)</span> 的最终表达为 <span
class="math display">\[ \mathbf{u}_i^{Final}=\sigma(\mathbf{W}\cdot
[Aggre^u(\mathbf{u}_i),\mathbf{u}_i])\]</span><br />
### <strong>Graph learning in hypergraph</strong>   给定一个user <span
class="math inline">\(u_i\)</span> 的会话上下文 <span
class="math inline">\(C=\{v_{c,1},v_{c,2},\cdots,v_{c,|C|}\}\)</span>,其表示
<span class="math inline">\(\mathbf{C}\)</span> 计算方式如下： <span
class="math display">\[\mathbf{C}_0=\mathbf{u}^{Final}\]</span> <span
class="math display">\[\mathbf{C}_t^*=LSTM(\mathbf{C}_{t-1})\]</span>
将user <span class="math inline">\(\mathbf{u}\)</span>
的隐表示作为初始向量 <span
class="math inline">\(\mathbf{C}_0\)</span>,然后使用LSTM迭代更新它。<span
class="math inline">\(\mathbf{C}_t^*\in \mathbb{R}^d\)</span> 表示 query
vector，用于计算对于item node <span
class="math inline">\(v_{c,i}\)</span> 的attention vector <span
class="math inline">\(e_{c,i,t}\)</span>。 <span
class="math display">\[e_{c,i,t}=f(\mathbf{v}_{c,i},\mathbf{C}_t^*)\]</span>
<span
class="math display">\[a_{c,i,t}=\frac{exp(e_{c,i,t})}{\sum_{c,j}exp(e_{c,j,t})}\]</span>
  item node <span class="math inline">\(v_{c,i}\)</span> 的 attention
vector <span class="math inline">\(e_{c,i,t}\)</span> 由 query vector
<span class="math inline">\(\mathbf{C}_t^*\)</span> 和 item node
的隐表示 <span class="math inline">\(\mathbf v_{c,i}\)</span>
共同决定，<span class="math inline">\(f\)</span> 是attention
function。然后计算注意力概率得分 <span
class="math inline">\(a_{c,i,t}\)</span> <span
class="math display">\[r_t=\sum_{c,i}a_{c,i,t}\mathbf{v}_i\]</span>
<span
class="math display">\[\mathbf{C}_t=Contact[\mathbf{C}_t^*,r_t]\]</span>
最后输出 <span class="math inline">\(\mathbf{C}_t\)</span> 作为user
session contex 隐表示用于下一item推荐。<br />
### <strong>Prediction and optimization</strong> <span
class="math display">\[\hat y_i=softmax((\mathbf
v_i)^T\mathbf{C})\]</span> <span
class="math display">\[Loss=-\sum_{i=1}^my_ilog(\hat
y_i)+(1-y_i)log(1-\hat y_i)\]</span></p>
<h2 id="experiment-setup">Experiment setup</h2>
<center>
表1：实验数据集统计表
</center>
<p><img src="table1.png" /></p>
<center>
表2：实验结果对比表
</center>
<p><img src="table2.png" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/16/testpage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="yogattt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/16/testpage/" class="post-title-link" itemprop="url">testpage</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-01-16 12:26:07 / Modified: 12:28:30" itemprop="dateCreated datePublished" datetime="2022-01-16T12:26:07+08:00">2022-01-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/hexo%E4%BD%BF%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">hexo使用</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>test the hexo work well after changing the device</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/16/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="yogattt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/16/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-16 10:55:47" itemprop="dateCreated datePublished" datetime="2022-01-16T10:55:47+08:00">2022-01-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/hexo%E4%BD%BF%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">hexo使用</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/15/Memory-Augmented-Multi-Instance-Contrastive-Predictive-Coding-for-Sequential-Recommendation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="yogattt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/15/Memory-Augmented-Multi-Instance-Contrastive-Predictive-Coding-for-Sequential-Recommendation/" class="post-title-link" itemprop="url">Memory Augmented Multi-Instance Contrastive Predictive Coding for Sequential Recommendation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-15 16:29:27" itemprop="dateCreated datePublished" datetime="2021-12-15T16:29:27+08:00">2021-12-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-16 10:55:47" itemprop="dateModified" datetime="2022-01-16T10:55:47+08:00">2022-01-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">推荐系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="abstract">Abstract</h2>
<p>  目前大多数序列推荐模型将下一个item预测任务作为训练信号。对于这些方法，有两个至关重要的挑战
1）难以捕捉用户的长期偏好。2）监督信号过于稀疏而无法有效训练模型。因此作者提出了一个名为MMInfoRec,基于记忆增强的多实例对比预测编码方案的框架来克服这些挑战。对比预测编码（Contrastive
Predictive Code
CPC）作为序列和item的编码器（encoder）.记忆模块旨在增强CPC中的自回归预测，以实现灵活和整体上（general）的偏好编码表示,从而提高捕获长期偏好的能力。为了有效训练MMInfoRec,作者提出了一个新颖的多实例噪声对比估计损失（muti-instance
noise constrastive estimation
MINCE）.该损失通过使用多个正样本来有效利用mini-batch内的样本。作者提出的框架属于对比学习类型。然而，鉴于其对比训练任务与目标推荐任务很好地对齐，因此不需要进一步地微调步骤。通过对四个基准数据集的大量实验，MMInfoRec
可以胜过最先进的基线。</p>
<h2 id="method">Method</h2>
<p><img src="figure1_overview.png" /></p>
<center>
图1：Overview of MMInfoRec
</center>
<h3 id="learning-framework"><strong>Learning Framework</strong></h3>
<p>  MMInfoRec的整体架构如图1所示。首先将序列中的所有item的id和attributes分别由两个嵌入层转换为密集特征向量。对于每一个item,它的特征由属性编码器
<span class="math inline">\(g_{enc}\)</span> 编码为潜在表示 <span
class="math inline">\(\boldsymbol z\)</span>。时间聚合模块(Temporal
aggegation) <span class="math inline">\(g_{ta}\)</span>
将序列信息编码为<span class="math inline">\(\boldsymbol
c_t\)</span>。通过自回归预测模块<span
class="math inline">\(g_{ap}\)</span>，<span
class="math inline">\(\boldsymbol c_t\)</span>
可用于预测未来的潜在表示。</p>
<p>1）<strong>Embedding Layyer</strong>：通过两个emdedding 矩阵：an item
embedding matrix <span class="math inline">\(\boldsymbol {Emb_i}\in
\mathbb{R}^{\rvert \mathcal{I} \rvert \times d}\)</span>,an attribute
embedding matrix <span class="math inline">\(\boldsymbol {Emb_A}\in
\mathbb{R}^{\rvert \mathcal{A} \rvert \times d}\)</span>。(其中 <span
class="math inline">\(\rvert \mathcal{I} \rvert\)</span>
为item的数量，<span class="math inline">\(\rvert \mathcal{A}
\rvert\)</span> 为数据集合中出现的item的属性的数量)将IDs和 attributes
转换为密集向量。 例如，物品 <span class="math inline">\(I_n\)</span>
及其属性<span class="math inline">\(\{
a_1,a_2\}\)</span>,通过lookup操作获得向量: <span
class="math inline">\(\mathbf x_n= \boldsymbol
{Emb_i}(i_n)\)</span>,<span class="math inline">\(\boldsymbol a_1=
\boldsymbol {Emb_A}(a_1)\)</span>,<span
class="math inline">\(\boldsymbol a_2= \boldsymbol
{Emb_A}(a_2)\)</span></p>
<p>2）<strong>Attribute Encoder</strong> :旨在将项目的所有信息（包括 ID
信息和属性信息）融合为潜在表示 $z $。 <span class="math display">\[
\mathbf z =\it{g}_{enc}\left(\mathbf x,\left\{\boldsymbol
a_*\right\}\right)\tag{1}\label{eq1}\]</span> 其中 <span
class="math inline">\(\mathbf x \in \mathbb{R}^{1\times
d}\)</span>,<span
class="math inline">\(\boldsymbol{a_*}\in\mathbb{R}^{1\times
d}\)</span>,<span class="math inline">\(\mathbf z \in\mathbb{R}^{1\times
d}\)</span>。 对于函数<span class="math inline">\(\it
g_{enc}\)</span>的实现，简单的mean,或AutoInt,或item
embedding到所有attribute embedding 的self attention
都适用于这个结构。作者使用的是Transformer</p>
<p>3）<strong>Temporal aggegation</strong>:时间聚合函数<span
class="math inline">\(\it
g_{ta}\)</span>用于聚合在某个特定时间之前的item Temporal
信息,得到context　vector。 <span class="math display">\[ \mathbf c_t =
\it{g}_{ta}(\mathbf z_1,\mathbf z_2,...,\mathbf z_t)\tag{2}\]</span>
其中<span class="math inline">\(\mathbf c_t\in\mathbb{R}^{1\times
d}\)</span>。可选择GRU或Attention作为时间聚合函数。作者使用Transformer。同时，作者将学习到的位置编码层添加到
<span class="math inline">\(\it g_{ta}\)</span> 的输入中。</p>
<p>4）<strong>Memory
Module</strong>:增强模型的表示能力，作者设计了一个记忆模块，根据context
vector计算每一步的预测输出。记忆模块中有一个带有b个内存条的内存库 <span
class="math inline">\(\mathbf M\in\mathbb{R}^{b \times d}\)</span>
内存寻址操作定义如下: <span class="math display">\[\hat {\mathbf
z}_{t+1} = \it g_m(\mathbf c_t) \]</span> <span
class="math display">\[=Softmax(MLP(\mathbf c_t)) \cdot\mathbf M +
\mathbf c_t \tag{3}\label{eq3}\]</span>
残差旨在保留原始预测并改进训练中的梯度流。</p>
<p>5）<strong>Auto-regressive
Prediction</strong>:对于多步预测任务，如果上下文被编码为向量 <span
class="math inline">\(\mathbf c_t\)</span> ,那么预测的潜在表示 <span
class="math inline">\(\hat{\mathbf z}_{t+*}\)</span> 预计与 <span
class="math inline">\(\mathbf c_t\)</span>
有很强的语义相似性。因此，我们引入自回归预测函数<span
class="math inline">\(\it g_{ap}\)</span> <span
class="math display">\[\mathbf c_{t+1}=\it{g}_{ap}(\hat{\mathbf
z}_{t+1}),\ \ \hat{\mathbf z}_{t+2} = \it{g}_m(\mathbf c_{t+1})
\tag{4}\]</span> 其中 <span class="math inline">\(\mathbf
c_{t+1}\)</span> 是时间步骤1到t+1的上下文摘要， <span
class="math inline">\(\hat{\mathbf z}_{t+2}\)</span> 是时间步骤 t + 2
的预测特征。类似于 Seq2Seq
预测风潜在表示以顺序方式一一预测。作者使用了GUR模型。</p>
<p>6）<strong>Recommendation</strong>:在MMInfoRec的验证和测试过程中，推荐是在序列表示
<span class="math inline">\(\mathbf c_t\)</span>
和项目集中所有项目之间的得分排序下进行的。 分数是通过当前时间步长 t
的序列表示 <span class="math inline">\(\mathbf c_t\)</span>
与项目集中所有项目的潜在表示 <span
class="math inline">\(\mathbf{z}\)</span> 之间的点积计算得出的。</p>
<h3 id="contrastive-loss"><strong>Contrastive Loss</strong></h3>
<p>Noise Contrastive Estimation (NCE)
是一种分类目标，可以区分真实样本和噪声样本。NCE
损失可以直接应用于项目预测任务。 在 MMInfoRec
中，作者将此损失扩展到多实例变体，可以有效缓解训练信号稀疏的问题。</p>
<p>1）<strong>Vanilla NCE Loss</strong>：vanilla NCE
主要对潜在空间中的表示进行比较，以迫使预测表示 <span
class="math inline">\(\hat {\mathbf z }\)</span> 接近真实表示 <span
class="math inline">\(\mathbf z\)</span> <span
class="math display">\[\mathcal{L}_{NCE}=-\sum_t\left[log {
e^{\left(\hat{\mathbf z}_t^T\cdot \mathbf z_t\right)/\tau}\over
e^{\left(\hat{\mathbf z}_t^T\cdot \mathbf
z_t\right)/\tau}+\sum_{*\in\mathcal{N}_i}e^{\left(\hat{\mathbf
z}_t^T\cdot \mathbf z_*\right)/\tau}}\right]\tag{5}\label{eq5}\]</span>
其中 <span class="math inline">\(\tau\)</span> 为温度参数，<span
class="math inline">\(\mathcal{N}_i\)</span>
是时间为t的item的负样本集。本质上 <span
class="math inline">\(\mathcal{L}_{NCE}\)</span>
是一个正样本对和其他所有负样本对之间的交叉熵损失</p>
<p>2）<strong>Negative Sampling within
Batch</strong>:因为训练时需要负样本集，因此需要负样本集的采样过程。一般有两种抽样方法：memory
bank and batch sampling。在 MMInfoRec 中，NCE
的计算在一个batch内进行，而不是在整个项目集中进行抽样，因为一个batch将包含足够的负样本。
在计算机视觉任务的 CPC
方法中，负样本可以从同一个特征图的通道中选择，也可以从同一批的其他特征图中选择。
类似地，在顺序推荐中，可以使用batch中其他物品的特征向量 <span
class="math inline">\(\mathbf{z}\)</span> 为每个物品i构造负样本集 <span
class="math inline">\(\mathcal{N}_i\)</span>。若 <span
class="math inline">\(D\)</span> 是train batch 中不同物品的数量,则 <span
class="math inline">\(\rvert \mathcal{N}_i\rvert=D-1.\)</span><br />
  如图一右边所示：定义General
negatives为同一batch中但不属于该序列的负样本，Temporal
negatives为同一序列中的负样本，模型更难区分。因为这些item和正样本在一个序列中，更倾向于代表用户的过时和误导性的偏好。所以把它们作为负样本训练模型是非常重要的，能够使模型区分差异.</p>
<p>3）<strong>Multiple Instance Positive
Sampling</strong>：对于预测的潜在表示 <span
class="math inline">\(\hat{\bf z}_t\)</span>
最自然的正样例对就是选择相应时间的真实潜在向量<span
class="math inline">\((\hat{\bf z}_t,\bf z_t)\)</span>
在本节，作者将这种正采样策略扩展到多实例方案。<br />
  将vanilla NCE
从单正样本扩展到语义正样本的多个实例，主要的难点是识别真实正样本的语义相关正样本。因为序列推荐通常将下一物item作为预测的标签，因此在NCE中一般很难像多个正样本那样找出语义相似的items.这种困难主要是由于基本无法定义序列的语义相似性，通常依赖于序列级别的最近邻计数。与计算机视觉研究领域的数据样本（例如图像和视频）不同，没有广泛适用的增强方法用于item的嵌入表示以获得语义正样本，也没有有意义的标签以监督学习的方式挖掘语义正样本。<br />
  为了解决语义相似正样本这一难题，MMInfoRec中提出了一种基于Droupout的正样本挖掘策略。如公式
<span class="math inline">\(\eqref{eq1}\)</span> 所描述的,item 由 <span
class="math inline">\(\it g_{enc}\)</span> 编码为encoder向量。如果在
<span class="math inline">\(\it g_{enc}\)</span>
中存在Droupout操作，通过对同一个item设置不同的Droupout
masks，就能得到这个item的一组 不同但语义相似的encoder向量。假定由 <span
class="math inline">\(q\)</span> 个不同的Droupout
函数，当这些函数在同一个item i
上操作时，得到的一组不同的隐向量表示为：<span
class="math inline">\(\mathcal P_i=\{\bf z_i^1,\bf z_i^2,\cdots,\bf
z_i^q, \}\)</span> 这些语义正样本在图1中表示为 Sem.Pos. 对batch
中的每一个item i 都生成一个这样的正语义样本集 <span
class="math inline">\(\mathcal P_i\)</span> ,因此multi-instance NCE
(MINCE) loss 定义如下: <span
class="math display">\[\mathcal{L}_{MINCE}=-\sum_t\left[log {
\sum_{*\in\mathcal{P}_i}e^{\left(\hat{\mathbf z}_t^T\cdot \mathbf
z_*\right)/\tau}\over \sum_{*\in\mathcal{P}_i}e^{\left(\hat{\mathbf
z}_t^T\cdot \mathbf
z_*\right)/\tau}+\sum_{*\in\mathcal{N}_i}e^{\left(\hat{\mathbf
z}_t^T\cdot \mathbf z_*\right)/\tau}}\right]\tag{6}\]</span> <span
class="math inline">\(\mathcal{N}_i\)</span>
是时间步骤为t的item的负样本集，它包含等式<span
class="math inline">\(\eqref{eq5}\)</span>负样本集中每个样本的q个变体。因此等式（6）中的负样本集合的大小为<span
class="math inline">\(q(D-1)\)</span>.</p>
<h2 id="experiment">Experiment</h2>
<h3 id="setup"><strong>Setup</strong></h3>
<p>1）<strong>Dataset</strong></p>
<pre><code>- Amazon Beauty, Sports, and Toys:将商品的细粒度分类和品牌作为属性。

- Yelp3：实验使用2019年1月1日之后的交易记录。将业务类别作为属性。</code></pre>
<p>2）<strong>Preprocessing</strong></p>
<p>  对于所有数据集，作者将来自同一用户的所有交互记录视为一个序列，根据交互时间排序。过滤出现次数少于5次的item以及长度小于5的序列。序列长度大于50的仅保留最近的50个交互记录。序列中的倒数第二项用于验证，最后一项用于测试，其余item用于训练。</p>
<center>
表1：Statistics of the datasets after preprocessing
</center>
<p><img src="table1.png" /></p>
<p>3）<strong>Hyper-Parameter</strong></p>
<p>  embedding size
设置为64，MMInfoRec中所有的线性映射函数都具有相同的隐藏大小。Transformer中层数设置为1，head设置为2。<span
class="math inline">\(\it g_{ta}\)</span> 的输入和 <span
class="math inline">\(\it g_{ta}\)</span>
内部的Transforme模块都使用ratio为0.5的Dropout函数，以缓解过拟合。batch
size设置为256。采用学习率分别为{0.0003，0.001，0.003,0.01,0.003}的Adam作为优化器。内存槽数量
<span class="math inline">\(b\)</span>
从{5，10，32，64，128，256}中选择。预测步数从 {1, 2, 3, 4}
中选择。用于多实例的Dropout
functions数量从{1，2，3，4}选择。温度参数从{0.1,0.3,0.6,1.3}。作者使用了
<span class="math inline">\(\mathcal{l}_2\)</span>
正则项用于训练。权重从{0，0.1，0.01，0.001，0.0001，0.00001}中选择。</p>
<h3 id="overall-performance"><strong>Overall Performance</strong></h3>
<center>
表2：Overall performance
</center>
<p><img src="table2.png" /></p>
<h3 id="ablation-study"><strong>Ablation Study</strong></h3>
<center>
表3：Ablation study for the memory module and the MINCE objective
</center>
<p><img src="table3.png" /></p>
<h3 id="memory-model"><strong>Memory model</strong></h3>
<p><img src="figure2.png" /></p>
<center>
图2：Results of different designs of memory module
</center>
<ul>
<li>None: 没有使用memory module</li>
<li><strong>FC-M</strong>:在等式 <span
class="math inline">\(\eqref{eq3}\)</span> 中去掉额外的原始上下文向量
<span class="math inline">\(\bf c_t\)</span> 。</li>
<li><strong>Res_M</strong>: 使用等式 <span
class="math inline">\(\eqref{eq3}\)</span> 中默认的带有残差结构的memory
module</li>
</ul>
<p>此外，作者将学习到的memory的范数进行了可视化。</p>
<p><img src="figure3.png" /></p>
<center>
图3：Visualisation of the norm of the memory learned by MMInfoRec on
Beauty with 64 slots and Sports with 10 slots.
</center>
<p>可以发现，学习到的内存库是一个稀疏矩阵。</p>
<h3 id="mince-in-sequential-recommendation"><strong>MINCE in Sequential
Recommendation</strong></h3>
<p><img src="figure4.png" /></p>
<center>
图4：Performance of different ranking objectives.
</center>
<h3 id="parameter-sensitivity"><strong>Parameter
Sensitivity</strong></h3>
<h4 id="impact-of-b"><strong>Impact of b:</strong></h4>
<p><img src="figure5.png" /></p>
<center>
图5：不同记忆槽数量b的实验结果
</center>
<p>  可以看出内存插槽的数量对整体性能影响不大。值得注意的是，根据图 3
中内存条的可视化结果，只有少量内存插槽对模型有贡献。
这可以解释，虽然整体内存插槽的数量变化很大，但性能仍然非常稳定。</p>
<h4 id="impact-of-tau"><strong>Impact of <span
class="math inline">\(\tau\)</span>:</strong></h4>
<p><img src="figure6.png" /></p>
<center>
图6：设置不同的温度参数的实验结果
</center>
<p>  从图中发现 <span class="math inline">\(\tau\)</span>
需要设置在一个合适的范围内才能达到合理的性能。如Sport和Yelp数据集，当
<span class="math inline">\(\tau\)</span>
太小时，对数值变得更接近确定性分布，这无法提供足够的训练信号来训练模型。</p>
<h3 id="impact-of-number-of-prediction-step"><strong>Impact of number of
prediction step:</strong></h3>
<p>本实验评估预测步数的影响，该参数控制自回归预测（auto-regressive
prediction） <span class="math inline">\(\it g_{ap}\)</span>
推出的步数以及将多少未来信息包含在训练中。</p>
<p><img src="figure7.png" /></p>
<center>
图7：设置不同的预测步数的实验结果
</center>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/10/Knowledge-Graph-Embedding-by-Translating-on-Hyperplanes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="yogattt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/10/Knowledge-Graph-Embedding-by-Translating-on-Hyperplanes/" class="post-title-link" itemprop="url">Knowledge Graph Embedding by Translating on Hyperplanes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-10 16:12:08" itemprop="dateCreated datePublished" datetime="2021-12-10T16:12:08+08:00">2021-12-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-03 17:33:11" itemprop="dateModified" datetime="2022-03-03T17:33:11+08:00">2022-03-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="abstract">abstract</h2>
<p>  我们通常将由实体和关系组成的知识图谱嵌入到连续的向量空间。TransE就是一种有效的方法。但是对于一些复杂关系的映射属性，如自反性、一对多、多对一和多对多，TransE并不能很好地处理这些属性。一些复杂的模型能够保留这些映射属性，但会在此过程中牺牲效率。
因此作者提出TransH，它将关系建模为超平面,并在其上进行平移操作。该方法能够以与transE几乎相同的复杂性,保留上述的关系映射属性。此外，由于实际知识图谱往往非常不完整，因此如何在训练中构建负例以降低错误负标签是非常重要的。
利用一对多/多对一关系映射属性，作者提出一个简单的技巧来减少错误负标签的可能性。作者在WordNet和Freebase等基准数据集上进行了大量的链接预测、三元组分类和知识抽取实验。实验表明，TransH在预测精度方面比TransE有显著的改进，并且具有可比的扩展能力。</p>
<h2 id="introduction">introduction</h2>
<p>  知识图谱（如Freebase <em>Bollacker et at. 2008</em>,WordNet
<em>Miler 1995</em> 和 GeneOntology <em>Ashburner et at.
2000</em>)已经成为了非常重要的资源以支持人工智能相关的应用，如网页/移动检索，Q&amp;A等等。一个知识图谱是一个多关系图,它由实体作为的节点和关系作为的不同类型的边组成。边的一个实例是事实的三元组（head
entity,relation,tail entity）(表示为(<span
class="math inline">\(h,r,t\)</span>)。在过去十年，有很多显著的成绩建立在大尺度的知识图谱上，但是支持计算的一般范式仍不明朗。两个主要的困难是：（1）知识图谱是一个符号逻辑系统而应用经常包括在连续空间的数字计算。(2)很难在一个图上聚合全局知识。形式逻辑推理的传统方法在处理真实大规模知识图谱上的长距离推理时，既不易理解也不可靠。一种方法是通过将一个知识图谱嵌入到连续的向量空间中同时保留原始图的某些属性，来解决这个问题<em>(Socher
et al. 2013; Bordes et al. 2013a; Weston et al. 2013; Bordes et al.
2011; 2013b; 2012; Chang, Yih, and Meek
2013)</em>。例如，每一个实体<span class="math inline">\(h\)</span>(or
<span class="math inline">\(t\)</span>)在向量空间中被表示为一个点<span
class="math inline">\(\textbf h\)</span>（or <span
class="math inline">\(\textbf t\)</span>）。而每一个关系<span
class="math inline">\(r\)</span>被建模为向量空间中以向量<span
class="math inline">\(\textbf
r\)</span>为特征的操作，例如平移，投影等等。实体和关系的表示是通过最小化一个包含所有实体和关系的全局损失函数得到的。因此，即使是单个实体/关系的嵌入表示，也会从整个知识图中对全局信息进行编码。然后，这些嵌入表示可以用来服务各种应用。一个直接的应用就是在知识图谱中计算缺失的边。对于任何候选三元组（<span
class="math inline">\(h,r,t\)</span>）我们可以简单通过检查表示<span
class="math inline">\(\textbf h\)</span>和<span
class="math inline">\(\textbf t\)</span>在以<span
class="math inline">\(\textbf
r\)</span>为特征操作下的兼容性来确认正确性。   </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/26/LaTex%E8%A1%A8%E8%BE%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="yogattt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/26/LaTex%E8%A1%A8%E8%BE%BE/" class="post-title-link" itemprop="url">LaTex表达</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-26 16:39:01" itemprop="dateCreated datePublished" datetime="2021-11-26T16:39:01+08:00">2021-11-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-16 10:55:47" itemprop="dateModified" datetime="2022-01-16T10:55:47+08:00">2022-01-16</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>命令\overbrace 和\underbrace 在表达式的上、下方给出一水平的大括号</p>
<ul>
<li><span
class="math inline">\(\underbrace{a+b+c+...+z}_{26个字母}\)</span>
：\underbrace{a+b+c+...+z}_{26个字母}</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/25/Unsupervised-Proxy-Selection-for-Session-based-Recommender-Systems/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="yogattt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/25/Unsupervised-Proxy-Selection-for-Session-based-Recommender-Systems/" class="post-title-link" itemprop="url">Unsupervised Proxy Selection for Session-based Recommender Systems</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-25 21:10:16" itemprop="dateCreated datePublished" datetime="2021-11-25T21:10:16+08:00">2021-11-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-03 17:28:21" itemprop="dateModified" datetime="2022-03-03T17:28:21+08:00">2022-03-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">推荐系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要">摘要</h2>
<p>   由于Session-based Recommender
Systems（SRS）中缺少用户相关信息，所以很难直接从数据中获得用户的整体兴趣。因此，现有的SRS侧重于如何在会话中有效地建模有关短期兴趣的信息，但它们不足以捕获用户的整体兴趣。为此，作者提出了一个新的框架来克服SRS的局限性，名为ProxySR，它通过对会话代理进行建模来模拟SRS中缺失的信息（即用户的整体兴趣）。<br />
 
 ProxySR以无监督的方式为输入的会话选择一个合适的proxy，将其与从该会话提取的短期兴趣结合，用于next
item
prediction。此外，作者还提出了SRS的另一种现实情况，即少数用户登录并在会话中留下他们的标识符，并针对这种情况修改了ProxySR。<br />
   ProxySR模型设计的初衷来源于以下总体兴趣的特征：</p>
<ul>
<li>多个会话可能具有相似的用户总体兴趣（比如，这多个会话是由同一个用户创建的，且该用户具备稳定的总体兴趣）</li>
<li>除了短期兴趣外，总体兴趣也可以弥补会话推荐中丢失的信息，以帮助预测下一个交互的item</li>
</ul>
<h2 id="methond">Methond</h2>
<figure>
<img src="figure1.png" alt="图1 ProxySR的总体架构" />
<figcaption aria-hidden="true">图1 ProxySR的总体架构</figcaption>
</figure>
<h3 id="proxy-selection"><strong>Proxy Selection</strong></h3>
<p>  ProxySR首先通过输入的session <span class="math inline">\(s\)</span>
构建偏态概率分布，进而从预先定义的K个proxy embeddings选择一个proxy
embedding。更具体地说，ProxySR利用encoder
network生成概率的对数，然后通过用具有温度参数的softmax函数将其转换为偏态概率分布
<span class="math inline">\(\pi \in \mathbb{R}^K\)</span>
，如下所示：<br />
<span class="math display">\[\boldsymbol\alpha=f^P(s)\]</span> <span
class="math display">\[\boldsymbol{\pi_i}={exp(\boldsymbol{\alpha_i}/\tau)\over
\sum_{j=1}^Kexp(\boldsymbol{\alpha_j}/\tau)} \ for\ i\in(1,2,...,K)
\tag{1}\label{eq1}\]</span> 其中 <span
class="math inline">\(f^P\in\mathbb{R}^K\)</span>是对于session <span
class="math inline">\(s\)</span> 的 encoder network。𝐾为Proxy数量，<span
class="math inline">\(\boldsymbol{\pi_i}\)</span>是第i个Proxy的概率。<span
class="math inline">\(\tau&gt;0\)</span>是温度参数。当<span
class="math inline">\(\tau\)</span>越小，<span
class="math inline">\(\boldsymbol\pi\)</span>变成一个接近one-hot向量的分布。当<span
class="math inline">\(\tau\)</span>越大，<span
class="math inline">\(\boldsymbol\pi\)</span>变成一个每个元素接近1/K的均匀分布。因此，作者将一个较大的初始值指定给𝜏
并随着训练的进行而减小，因为如果𝜏
很小，梯度会偏向于少数对数，这对于不稳定的初始训练是不可取的。最后获得session
<span class="math inline">\(s\)</span>的proxy embedding <span
class="math inline">\(p^{(s)}\)</span><br />
<span class="math display">\[
\gamma={\sum_{j=1}^K\Vert\boldsymbol{\pi_j}P_j\Vert_2\over
\Vert\sum_{j=1}^K\boldsymbol{\pi_j}P_j\Vert_2}
\]</span> <span
class="math display">\[p^{(s)}=\gamma\sum_{j=1}^k\boldsymbol{\pi_j}P_j\tag{2}\]</span></p>
<p>其中 <span class="math inline">\(P\in\mathbb{R}^{K\times d}\)</span>
是proxy embedding matrix。<span class="math inline">\(\tau\)</span>
经过几次训练后已经足够小了，因此 <span
class="math inline">\(\pi\)</span> 变为一个one-hot
vector，并且仅从集合中选择一个proxy embedding。<br />
  当<span
class="math inline">\(\tau\)</span>在初始训练阶段较大时，获得的代理的规模可能太小，因为每个proxy都是以均值为
0 随机初始化的，并均匀聚合以相互抵消。因此，作者通过使用 <span
class="math inline">\(\gamma\)</span>
重新缩放获得的proxy来防止这个问题，这迫使其 <span
class="math inline">\(\mathcal{l}_2\)</span> 范数保持proxies的 <span
class="math inline">\(\mathcal{l}_2\)</span>范数的加权平均值。</p>
<h4 id="boldsymbolfps实现细节"><strong><span
class="math inline">\(\boldsymbol{f^P(s)}\)</span>实现细节</strong></h4>
<p>  encoder network如下<br />
<span class="math display">\[f^P(s)={1\over n}
\sum_{j=1}^nW^{P,(2)^T}\sigma\left(W^{P,(1)^T}\left(I_{s_j}+E_j^P\right)\right)\tag{3}\label{eq3}\]</span><br />
其中，<span class="math inline">\(I_{s_j}\)</span>是session <span
class="math inline">\(s\)</span>中第j个item的embedding。<span
class="math inline">\(E_j^P\)</span>是对于位置j的learnable positional
embedding。<span
class="math inline">\(W^{P,(1)^T}\in\mathbb{R}^{d\times\lfloor
(d+K)/2\rfloor }\)</span>,<span
class="math inline">\(W^{P,(2)^T}\in\mathbb{R}^{\lfloor (d+K)/2\rfloor
\times K }\)</span> 为权重矩阵。<span
class="math inline">\(\sigma\)</span>为负斜率为0.1的Leaky ReLU。<br />
  注意,在训练阶段，作者使用了全部的session item来选择proxy，而在预测next
item <span class="math inline">\(s_t\)</span>时只使用了<span
class="math inline">\([s_1,s_2,...s_{t-1}]\)</span></p>
<h4 id="discussion"><strong>Discussion</strong></h4>
<p>  作者讨论了为什么不使用普通的softmax将多个proxies加权生成对应session的proxy。作者认为，对多个proxies的加权组合会为每个session创建一个独一无二的proxy,这相当于对session
encoder 为representation。但是，只从单个session
中很难提取整体兴趣。因此不能保证加权结合能够对多个sessions中的公共整体兴趣建模。而ProxySR选择的是最有可能的一个proxy，因此选择的proxy能够在多个session之间共享，进而能捕捉这些sessions之间的共同信息。</p>
<h3 id="short-term-interest-encoder"><strong>Short-term Interest
Encoder</strong></h3>
<p>  session 中本身包含短期兴趣，因此直接将session encoder
为隐表示作为short-term interest <span
class="math inline">\(s^{(s)}\)</span><br />
<span class="math display">\[
s^{(s)}=f^S(s)\tag{4}\label{eq4}
\]</span> 其中<span
class="math inline">\(f^S(s)\in\mathbb{R}^d\)</span>为encoder
network。<span class="math inline">\(s^{(s)}\in\mathbb{R}^d\)</span>
为session <span class="math inline">\(s\)</span>包含的short-term
interest的表示。</p>
<h4 id="boldsymbolfss实现细节"><strong><span
class="math inline">\(\boldsymbol{f^S(s)}\)</span>实现细节</strong></h4>
<p>  考虑items之间的依赖关系，作者使用具有残差连接的自注意力网络。 <span
class="math display">\[X=[I_{s_1}+E_n^S,I_{s_2}+E_{n-1}^S,...,I_{s_n}+E_1^S]^T
\]</span> <span class="math display">\[Q=ReLU(XW^{S,(Q)})\]</span> <span
class="math display">\[K=ReLU(XW^{S,(K)})\]</span> <span
class="math display">\[A=softmax({QK^T\over\sqrt d })\]</span> <span
class="math display">\[Z=AX+X \]</span> <span
class="math display">\[f^S(s)=W^{S,(2)^T}ReLU\left(W^{S,(1)^T}Z+b^{S,(1)}\right)+b^{S,(2)}\tag{5}\label{eq5}\]</span>
其中<span class="math inline">\(E_j^S\)</span>为逆序的learnable
positional embedding。<span
class="math inline">\(X\in\mathbb{R}^{n\times
d};W^{S,(Q)},W^{S,(K)},W^{S,(1)},W^{S,(2)}\in\mathbb{R}^{d\times
d};b^{S,(1)},b^{S,(2)}\in\mathbb{R}^d\)</span></p>
<h3 id="combination"><strong>Combination</strong></h3>
<p>  该部分将为session s选择好的proxy <span
class="math inline">\(p^{(s)}\)</span>和其短期兴趣<span
class="math inline">\(s^{(s)}\)</span>结合到一起，获得会话s的最终表示，然后再用该表示计算该session和目标item
i之间相异性得分。然而，根据一些先例研究，简单的加法不能模拟比一对一关系更复杂的三元组内的关系。</p>
<ul>
<li>情景1：如果同一个item与两个不同和短期兴趣、同一个selected
proxy相关，那么模型可能会认为这两个不同的短期兴趣是相似的，即if <span
class="math inline">\(p + s^{(1)} ≈ I_i\)</span> and <span
class="math inline">\(p + s^{(2)} ≈ I_𝑖\)</span> then $ s^{(1)} ≈
s^{(2)}$</li>
<li>情景2：如果两个不同的item与同一个proxy相关，且分别与两个相似的短期兴趣相关，那么模型会认为这两个不同的item是相似的，即
if <span class="math inline">\(p + s^{(1)} ≈ I𝑖^{(1)}\)</span> and <span
class="math inline">\(p + s^{(2)} ≈ I_i^{(2)}\)</span> where <span
class="math inline">\(s^{(1)} ≈ s^{(2)}\)</span>, then <span
class="math inline">\(I_i^{(1)} ≈ I_i^{(2)}\)</span></li>
</ul>
<p>因此，作者借鉴TransH的思想，将短期兴趣和目标item的embedding投影到超平面（hyperplane），以捕获三元组间复杂的关系。具体来说，首先获得投影到超平面的short-term
interest <span class="math inline">\(s_\perp ^{(s)}\)</span>以及目标
item embedding <span class="math inline">\(I_{i\perp}\)</span> <span
class="math display">\[
\boldsymbol{v}={\sum_{j=1}^K\Vert\boldsymbol{\pi_j}V_j\Vert_2\over
\Vert\sum_{j=1}^K\boldsymbol{\pi_j}V_j\Vert_2}
\]</span> <span class="math display">\[s_\perp
^{(s)}=s^{(s)}-\boldsymbol{v}^Ts^{(s)}\boldsymbol{v}\]</span> <span
class="math display">\[I_{i\perp}=I_i-\boldsymbol{v}^TI_i\boldsymbol{v}\]</span>
<span class="math inline">\(V\in\mathbb{R}^{K\times d}\)</span> 是proxy
hyperplanes的单位法向量set。<span
class="math inline">\(\boldsymbol{v}\in\mathbb{R}^d\)</span>是投影到<span
class="math inline">\(p^{(s)}\)</span>的超平面的单位法向量。为了使法向量与proxy的超平面正交并具有单位长度，作者约束<span
class="math inline">\(\lvert v\cdot p^{(s)}\rvert/\Vert p^{(s)}\Vert_2
\le \epsilon\)</span> , <span class="math inline">\(\Vert
V_j\Vert_2=1\)</span><br />
  最后，会话 𝑠 和目标item 𝑖 之间的相异性分数是通过预测的item
embedding与proxy的聚合以及预测的短期兴趣之间的距离来估计的。计算相异性分数如下：
<span
class="math display">\[dist(s,i)=\left\Vert\left(p^{(s)}+s_\perp^{(s)}\right)-I_{i\perp}\right\Vert_2^2\tag{7}
\]</span></p>
<h3 id="training"><strong>Training</strong></h3>
<p>  采用 marginal loss 训练模型。采用单位法向量 v
的正交正则器orthogonality regularizer和distance
regularizer，强制会话表示接近目标item embbedng.<br />
  首先定义损失函数<span class="math inline">\(\mathcal{L}\)</span> <span
class="math display">\[\mathcal{L}=\sum_{\{s,i^+\}\in \boldsymbol
S}\sum_{i^-\in NI(s)}[m+dist(s,i^+)-dist(s,i^-)]_+\tag{8}
\]</span> <span class="math inline">\(i^+\)</span> 为session s的true
next item, <span class="math inline">\(NI(s)\subset I\backslash
i^+\)</span> 是session s 的 negative items 集。<span
class="math inline">\([x]_+=max(x,0)\)</span>。𝑚 is the margin。<br />
  包括正则化项，最终的最小化目标函数J定义为如下所示：<br />
<span class="math display">\[reg^{dist}=\sum_{\{s,i^+\}\in \boldsymbol
S}dist(s,t^+)\]</span> <span
class="math display">\[reg^{orthog}=\sum_{\{s,i^+\}\in \boldsymbol
S}{\rvert v^{(s)}\cdot p^{(s)}\rvert\over \Vert p^{(s)}\Vert_2}\]</span>
<span class="math display">\[\mathcal{J}=\mathcal{L}+\lambda^{dist}\cdot
reg^{dist}+\lambda^{orthog}\cdot reg^{orthog}\tag{9}\]</span></p>
<h3 id="another-real-world-case-user-semi-supervision"><strong>Another
Real-world Case: User Semi-supervision</strong></h3>
<p>作者还考虑了当数据集中存在部分用户信息的时候，如何利用这些用户信息进行半监督学习。即在生成proxy的概率分布时加入user
bias，对于缺少用户信息的session,仍按初始的方式计算proxy的概率<span
class="math inline">\(\eqref{eq1}\)</span>： <span
class="math display">\[\boldsymbol{\pi_i}^{user}={exp\left(\left(\boldsymbol{\alpha_i}+u_j^{(s)}\right)/\tau\right)\over
\sum_{j=1}^Kexp\left(\left(\boldsymbol{\alpha_j}+u_j^{(s)}\right)/\tau\right)}
\ for\ i\in(1,2,...,K) \tag{10}\label{eq10}\]</span><br />
其中<span class="math inline">\(u^{(s)}\in\mathbb{R}^K\)</span>
是对于session s的用户可学习user bias。</p>
<h2 id="experiments">EXPERIMENTS</h2>
<h3 id="datasets"><strong>datasets</strong></h3>
<center>
表1 Statistics of datasets
</center>
<p><img src="table1.png" /></p>
<p>实验包含两个任务：</p>
<ul>
<li>next unseen item recommendation</li>
<li>next item recommendation with repetitive consumption.</li>
</ul>
<h3 id="performance-comparison"><strong>Performance
Comparison</strong></h3>
<center>
表2 Overall performance on the next unseen item recommendation
</center>
<p><img src="table2.png" /></p>
<center>
表3 Overall performance on the next item recommendation with repetitive
consumption
</center>
<p><img src="table3.png" /></p>
<p>ProxySR与CSRM和GCE-GNN的比较证明，基于item
co-occurrence的相邻会话的信息不足以捕获会话的一般兴趣。
ProxySR在短序列的数据集更有效。如在RetailRocket数据集上提升最大。在LastFM数据集上提升最小。</p>
<center>
表4 Performance of ProxySR in the real-world scenario where a few
sessions have their user information
</center>
<p><img src="table4.png" /></p>
<h3 id="消融实验"><strong>消融实验</strong></h3>
<center>
表5 Result of the ablation study on each component in ProxySR
</center>
<p><img src="table5.png" /></p>
<h3 id="hyperparameter-study"><strong>Hyperparameter Study</strong></h3>
<figure>
<img src="figure3.png" alt="图2" />
<figcaption aria-hidden="true">图2</figcaption>
</figure>
<center>
图2 Result of the hyperparameter parameter study on 𝐾 in ProxySR.
</center>
<h3 id="analyses-on-proxies-from-proxysr"><strong>Analyses on Proxies
from ProxySR</strong></h3>
<h4 id="information-encoded-in-proxies"><strong>Information Encoded in
Proxies</strong></h4>
<center>
表6 Performance of HRNN with various types of the user information in it
</center>
<p><img src="table6.png" /><br />
  HRNN通过user-level RNN，顺序使用用户会话来训练user embedding。</p>
<h4 id="visualizations"><strong>Visualizations</strong></h4>
使用t-分布领域嵌入（t-SNE）来可视化高维表示。图3展示了与10个随机用户相关的session表示可视化，相同颜色的圆圈表示该session属于同一个user。
<img src="figure4.png" alt="图3" /><br />
<center>
图3 Visualizations of several representations related to sessions
</center>
<p>从图3可以发现：</p>
<ul>
<li>有些proxy是由多个用户的会话选择的，这是因为多个用户可能有相似的一般兴趣（多种颜色聚集的一团）</li>
<li>多个proxies被相同用户的session选择，说明proxy可以对比用户一般兴趣更细粒度的信息进行建模。（浅蓝色那一团）</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/16/Learning-Feature-Interactions-with-Lorentzian-Factorization-Machine/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="yogattt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/16/Learning-Feature-Interactions-with-Lorentzian-Factorization-Machine/" class="post-title-link" itemprop="url">Learning Feature Interactions with Lorentzian Factorization Machine</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-16 17:02:14" itemprop="dateCreated datePublished" datetime="2021-11-16T17:02:14+08:00">2021-11-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-16 10:55:47" itemprop="dateModified" datetime="2022-01-16T10:55:47+08:00">2022-01-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">推荐系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要">摘要</h2>
<p>   
学习特征交互的表示以模拟用户行为对于推荐系统和点击率（CTR）预测至关重要。深度学习方法能够学习复杂的特征交互,但是这些方法需要大量与low-level
representations相结合的训练参数，因此内存和计算效率都很低。</p>
<p> 
 作者提出了一个名为“LorentzFM”的新模型，该模型可以学习嵌入在双曲空间（hyperbolic
space）中的特征相互作用，在双曲空间中，Lorentz距离的三角不等式的破坏是可实现的。双曲三角形的特殊几何特性对学习特征交互的表示是有益的。并且因为不需要任何顶部深度学习层，参数数量显著减少（20%至80%）。</p>
<p>  作者提出在使用Lorentz距离的双曲空间中学习低维表示，这样就能违反特征向量的三角不等式。该想法是受collaborative
metric learning (CML) (Hsieh et al. 2017)
工作的启发。不同的是，CML作者认为欧氏空间中的三角形不等式应该严格遵守，本文作者提出利用三角形不等式的符号。具体地说，本文作者不是通过特征向量之间的内积或距离，而是通过检查它们在双曲空间中形成的三角形来构造特征交互的分数函数。采取这种做法的原因有两个方面：<br />
    1.双曲空间本质上比欧几里德空间更广阔；<br />
    2.提出的分数函数将提供一个鲁棒的目标函数来学习细粒度特征交互。</p>
<h2 id="技术背景">技术背景</h2>
<h3 id="双曲几何hyperbolic-geometry"><strong>双曲几何(Hyperbolic
Geometry)</strong></h3>
<p><a
target="_blank" rel="noopener" href="https://www.cnblogs.com/baiting/p/11006331.html">推荐阅读</a><br />
  
双曲几何旨在研究具有常数负曲率的非欧几里德空间。由于其负曲率，双曲几何与欧几里德几何相比具有非常不同的性质。<br />
  
首先，与欧几里德空间中的线性和二次增长率相反，双曲空间中圆的周长和面积随半径呈指数增长。因此，在半径上界相同的双曲空间中嵌入的容量比欧几里德空间中嵌入的容量大得多。其次，定义在洛伦兹距离下的三角不等式是可以违反。这个性质使我们能够用不等式的符号来刻画双曲空间中点之间的成对关系。（双曲空间有几个重要的计算模型：
the Poincare ball model, the hyper-boloid model, the Klein model）<br />
<strong>Hyperboloid Model</strong><br />
  定义 <span class="math inline">\(\textbf{u},\textbf{v}\in
\mathbb{R}^{n+1}\)</span> 之间的洛伦兹内积如下：<br />
<span class="math display">\[\langle \textbf{u},\textbf{v}
\rangle_{\mathcal{L}} = -u_0v_0+\sum_{i=1}^nu_iv_i\tag{1} \label{eq1}
\]</span>   n维双曲面 <span
class="math inline">\(H^{n,\beta}\subseteq\mathbb{R}^{n+1}\)</span>由以下定义的点集组成：</p>
<p><span class="math display">\[H^{n,\beta}=\lbrace\textbf{x}\in
\mathbb{R}^{n+1}:\Vert \textbf{x}
\Vert_{\mathcal{L}}^2=-\beta,x_0&gt;\beta \rbrace \tag{2}
\label{eq2}\]</span></p>
<p>  <span class="math inline">\(\Vert \textbf{x}
\Vert_{\mathcal{L}}^2=\langle \textbf{x},\textbf{x}
\rangle_{\mathcal{L}}\)</span>
表示向量X的洛伦兹范数。在此定义下，每个向量 <span
class="math inline">\(\textbf{x}\in H^{n,\beta}\)</span> 的第0维度 <span
class="math inline">\(x_0\)</span> 不能随意指定，应由以下公式定义：
<span class="math display">\[x_0=\sqrt{\beta+\sum_{i=1}^n x_i}\tag{3}
\label{eq3}\]</span>   两点之间的相关测地距离为:<br />
<span class="math display">\[d_l(
\textbf{u},\textbf{v})=arccosh(-\langle\textbf{u},\textbf{v}\rangle_{\mathcal{L}})
\tag{4}\label{eq4}\]</span><br />
  请注意，双曲面模型 <span class="math inline">\(H^{n,\beta}\)</span>
的原点向量为 <span
class="math inline">\(\boldsymbol{0}=(\beta,0,...,0)\)</span> 并且 <span
class="math inline">\(\boldsymbol{0}\)</span> 与任意向量 <span
class="math inline">\(\textbf{x}\in \mathbb{R}^{n+1}\)</span>
的洛伦兹内积定义为 <span
class="math inline">\(\langle\textbf{0},\textbf{x}\rangle=-x_0&lt;\beta\)</span>.这里原点向量的定义和原点向量与其他向量的内积应该是特殊定义，因为它们分别不满足等式
<span class="math inline">\(\eqref{eq2}\)</span> 和 <span
class="math inline">\(\eqref{eq1}\)</span><br />
  当 <span
class="math inline">\(\beta=1\)</span>时，该模型称为单位双曲面模型（unit
Hyperboloid Model）。这将贯穿整个论文，并且将 <span
class="math inline">\(H^{n,1}\)</span> 简记为 <span
class="math inline">\(H^{n}\)</span>。</p>
<p><strong>Lorentz Distance</strong><br />
  <span class="math inline">\(\textbf{u}\)</span> , <span
class="math inline">\(\textbf{v} \in H^n\)</span>
之间的平方洛伦兹距离（简称洛伦兹距离）定义如下：<br />
<span
class="math display">\[d_{\mathcal{L}}^2(\textbf{u},\textbf{v})=\Vert
\textbf{u}-\textbf{v}\Vert_{\mathcal{L}}^2=-2-2\langle\textbf{u},\textbf{v}\rangle_{\mathcal{L}}\tag{5}\label{eq5}\]</span><br />
  它几乎满足欧几里得几何的所有公理，但是不满足三角不等式。三角不等式是正定黎曼度量的最关键几何性质之一，它表明对于任意三个点
<span class="math inline">\(\textbf{x}\)</span>,<span
class="math inline">\(\textbf{y}\)</span>,<span
class="math inline">\(\textbf{z}\)</span>，任意两对点之间的距离<span
class="math inline">\(d(.,.)\)</span>的和大于或等于剩下一个点对之间的距离。
<span class="math display">\[d(\textbf{x},\textbf{y})\leq
d(\textbf{x},\textbf{z})+d(\textbf{z},\textbf{y})\tag{6}\label{eq6}\]</span>
  在双曲空间中，公式<span
class="math inline">\(\eqref{eq4}\)</span>定义的测地距离满足此不等式，但是在洛伦兹距离<span
class="math inline">\(\eqref{eq5}\)</span>下,可能不满足此不等式，因为黎曼度量是负的。考虑原点和两个点<span
class="math inline">\(\textbf{u}\)</span>,<span
class="math inline">\(\textbf{v}\)</span>组成的三角形，如图1所示。当两个点在<span
class="math inline">\(x_1\)</span>轴不同边相距很远时，违反了三角不等式。如果两个点在同<span
class="math inline">\(x_1\)</span>轴一边，三角不等式成立。</p>
<img src="triangle.png" alt="图一" /><br />
<center>
图1 (a)违反了三角不等式,（b）满足三角不等式 <span
class="math inline">\(\label{pic1}\)</span>
</center>
<h3 id="learning-triangle-inequalities"><strong>Learning Triangle
Inequalities</strong></h3>
  Hsieh et al.
(2017)指出学习嵌入空间中的距离而不是内积有利于学习细粒度的嵌入空间，该空间不仅可以捕获item-user交互的表示，还可以捕获item-item和user-user距离的表示。本质上，所谓的度量学习方案(metric
learning scheme)受到三角不等式的约束。<br />
  与协同度量学习方案相反，作者认为两点之间的特征交互可以通过洛伦兹距离的三角不等式的符号来学习，而不是使用距离本身。形式上，score
function写为： <span class="math display">\[
\mathcal{T}(\textbf{x},\textbf{y})={d_{\mathcal{L}}^2(\textbf{x},\textbf{y})-d_{\mathcal{L}}^2(\textbf{x},\textbf{0})-d_{\mathcal{L}}^2(\textbf{0},\textbf{y})
\over
\langle\textbf{0},\textbf{x}\rangle_{\mathcal{L}}\langle\textbf{0},\textbf{y}\rangle_{\mathcal{L}}}\tag{7}\label{eq7}
\]</span><br />
分子为不等式两边的差，分母是为了约束score function.<br />
  该score function
对于所有的维度，取值范围均为[-0.5,2].因此，与协同度量学习方案相比，分数函数不受维度的影响。如图2（a），为了说明函数的布局，绘制了等式（7）在2D中两点的取值。<br />
<img src="2D.jpg" alt="图2" /><br />
<center>
图2 （a）triangle learning方案（例如，等式 <span
class="math inline">\(\eqref{eq7}\)</span>
）和（b）在二维双曲面模型中使用测地距离的度量学习方案中分数函数的二维图
</center>
<p>因为二维双曲模型中的点只有一个自由参数（参考<span
class="math inline">\(\eqref{eq3}\)</span>），因此图中的x轴和y轴分别表示两个点的自由参数。同时，作者使用等式<span
class="math inline">\(\eqref{eq4}\)</span>中定义的测地距离绘制了协同度量学习方案的取值图2（b）。通过比较，可以观察到作者提出的方法的取值是平滑和有界的，但是协作度量学习方案的得分函数取值是无界的。有界性是有用的，因为嵌入向量可以自由地远离原点，而分数函数仍可以平滑增长。</p>
<h2 id="lorentzian-factorization-machine">Lorentzian Factorization
Machine</h2>
<h3 id="overview"><strong>Overview</strong></h3>
<img src="pic3.jpg" alt="图3" /><br />
<center>
图3 LorentzFM结构图
</center>
<p>  如图三所示，稀疏特征向量<span
class="math inline">\(\mathcal{V}_x\)</span>作为模型输入。经过Lorentz
embedding
layer，将所有特征投影到同一个双曲空间。接下来，将所有字段的嵌入引入一个新的triangle
pooling层，该层作为所有特征对的聚合函数，从整体上度量三角形不等式的 soft
“validness”
。与最近建立在欧几里德嵌入基础上的最先进的神经结构不同，LorentzFM不需要任何额外的参数。特别是，对于给定的稀疏输入<span
class="math inline">\(V_x\)</span>，池化层的输出是模型输出分数<br />
<span
class="math display">\[\hat{S}_{LFM}(\mathcal{V}_x)=\sum_{i,j=1,i\neq
j}^d \mathcal{T} (\textbf{v}_i,\textbf{v}_j)x_ix_j \tag{8}\label{eq8}
\]</span><br />
其中<span class="math inline">\(\textbf{v}_i,\textbf{v}_j \in
H^n\)</span>是每个输入特征字段的嵌入向量。<span
class="math inline">\(\mathcal{T}(.,.)\)</span>是特征交互函数。虽然在公式（8）中形式上缺少线性项，但它实际上在池函数<span
class="math inline">\(\mathcal{T}(.,.)\)</span>中重新出现，如下文所示。（为什么作者强调要有线性项）</p>
<h3 id="lorentz-embedding-layer"><strong>Lorentz Embedding
Layer</strong></h3>
<p>  嵌入层是一个lookup操作，用于将稀疏特征投影到洛伦兹空间中的低维密集向量。即<span
class="math inline">\(\textbf{v}_k\in
H^n\)</span>是第k个特征的嵌入向量，而第0个分量由等式<span
class="math inline">\(\eqref{eq3}\)</span>中的约束给出。<br />
  在某些情况下，分类特征可以是多值的。例如，电影《泰坦尼克号》的类型可以是“Drama”或“Romance”。因此为这些分类特征使用多个字段，并用“unknown”填充它们，以确保每个样本在特征维度上对齐。</p>
<h3 id="triangle-pooling-layer"><strong>Triangle Pooling
Layer</strong></h3>
<p>  Pooling 层是一个聚合函数，用于将一组嵌入向量转换为一个向量： <span
class="math display">\[
\mathcal{T}(\textbf{u},\textbf{v})={d_{\mathcal{L}}^2(\textbf{u},\textbf{v})-d_{\mathcal{L}}^2(\textbf{u},\textbf{0})-d_{\mathcal{L}}^2(\textbf{0},\textbf{v})
\over
2\langle\textbf{0},\textbf{u}\rangle_{\mathcal{L}}\langle\textbf{0},\textbf{v}\rangle_{\mathcal{L}}}
\]</span> <span class="math display">\[
={1-\langle\textbf{u},\textbf{v}\rangle_{\mathcal{L}}-u_0-v_0 \over
u_0v_0}
\]</span> <span class="math display">\[
={1-\langle\textbf{u},\textbf{v}\rangle_{\mathcal{L}} \over
2u_0v_0}-\left({1\over u_0}+{1\over v_0}\right)\tag{9}\label{eq9}
\]</span></p>
<p>由于标准化分母，出现了线性项，如公式最后一行<span
class="math inline">\(\left({1\over u_0}+{1\over
v_0}\right)\)</span>所示。</p>
<h3 id="objective-and-learning"><strong>Objective and
Learning</strong></h3>
<p>  推荐系统和CTR预测的目标函数是二元交叉熵（BCE）： <span
class="math display">\[
\arg\,\min_{\theta}\sum_i-y_ilog(p_i)-(1-y_i)log(1-p_i)\tag{10}\label{eq10}
\]</span> i表示第i个样本，其中<span
class="math inline">\(p_i=\sigma\left(\hat{S}_{LFM}\left(\mathcal{V}_x^i\right)\right)\)</span>,为第i输入样本<span
class="math inline">\(\mathcal{V}_x^i\)</span>的（点击）可能性。<span
class="math inline">\(y_i\)</span>是真实标签。作者指出，尽管贝叶斯个性化排名（Bayesian
Personalized Ranking, BPR）损失（Rendle et
al.2009）在普遍的推荐系统中被证明是有用的，但作者不使用它，因为BCE损失是符号敏感的，这是期望的属性，而BPR损失不是。<br />
  模型参数是通过使用黎曼随机梯度下降法（RSGD）（Bonnabel
2013）学习的。如Nickel和Kiela（2018）提到的，参数更新方式如下所示：
<span class="math display">\[
\theta_{t+1}=exp_{\theta_t}(-\eta\ grad\
f(\theta_t))\tag{11}\label{eq11}
\]</span> 其中，<span class="math inline">\(grad\
f(\theta_t)\)</span>是黎曼流形中定义的梯度，<span
class="math inline">\(\eta\)</span>是学习率。黎曼梯度是通过将欧几里德空间中的梯度乘以洛伦兹度量，然后在当前参数集跨越的切线空间上执行正交投影来获得的。最后，通过以下指数映射给出参数更新:
<span class="math display">\[
exp_{\theta_t}(\textbf{x})=cosh(\Vert\textbf{v}\Vert_{\mathcal{L}})\textbf{x}+sinh(\Vert\textbf{v}\Vert_{\mathcal{L}}){\textbf{v}\over
\Vert\textbf{v}\Vert_{\mathcal{L}}}\tag{12}\label{eq12}
\]</span> 它将切线空间中的切线向量v映射到洛伦兹流形上。详细信息见Nickel
and Kiela（2018） Learning continuous hierarchies in the lorentz model
of hyperbolic geometry。(待学习)<br />
  由于score function等式<span
class="math inline">\(\eqref{eq7}\)</span>是有界且与维度无关的，因此无需在嵌入向量上应用L2正则化项，因为它会对双曲面模型的原点创造一个抵抗梯度。</p>
<h2 id="实验">实验</h2>
<figure>
<img src="table1.jpg" alt="表1" />
<figcaption aria-hidden="true">表1</figcaption>
</figure>
<center>
表1
数据预处理后的数据集统计。在Avazu数据集中，itemID和userID没有明确的指示符，因此将相应的行留空。
</center>
<p>Steam, MovieLens and KKBox数据集用于推荐任务，Avazu用于CTR任务。</p>
<figure>
<img src="table2.jpg" alt="表2" />
<figcaption aria-hidden="true">表2</figcaption>
</figure>
<center>
表2 每个模型在测试集上的最佳性能。
</center>
<p>从表2可发现，
除了MovieLens数据集，LorentzFM的效果比其他模型都好。原因是MovieLens的稀疏特征的数量是其他数据集的5到100倍，说明当数据非常稀疏时，LorentzFM功能尤其强大。</p>
<figure>
<img src="table3.jpg" alt="表3" />
<figcaption aria-hidden="true">表3</figcaption>
</figure>
<center>
表3 在最佳性能下，每个模型的训练参数数量与训练时间比较表。
</center>
<figure>
<img src="figure5.jpg" alt="图5" />
<figcaption aria-hidden="true">图5</figcaption>
</figure>
<center>
图5 Visualization of the heatmap of score functions from LorentzFM for
(a) a positive sample and (b) a negative sample, and from FM for the
same (c) positive sample and (d) negative sample
</center>
<p>  作者选了一个典型的用户(user ID
“76561198071045315”)。该用户几乎所有正评分都是Steam中免费的游戏。针对该用户，作者画了图5。从图5(a)中可以发现，对于游戏是免费的正样本，由LorentzFM学习到的“UserID”
和
“Price”的特征交互分数达到了最高0.4。说明用户非常倾向于免费的物品。而从图5（c）并不能得到该信息。<br />
  对于游戏价格为$14.99的负样本，由LorentzFM学习到的“UserID” 和
“Price”的特征交互分数几乎是最低的负数。这意味着负样本主要是由于其价格而被识别的。而FM中“UserID”
和 “Price” 的内积结果并不占主要地位。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/15/Multi-Interactive-Attention-Network-for-Fine-grained-Feature-Learning-in-CTR-Prediction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="yogattt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/15/Multi-Interactive-Attention-Network-for-Fine-grained-Feature-Learning-in-CTR-Prediction/" class="post-title-link" itemprop="url">Multi-Interactive Attention Network for Fine-grained Feature Learning in CTR Prediction</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-15 13:27:18" itemprop="dateCreated datePublished" datetime="2021-11-15T13:27:18+08:00">2021-11-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-16 10:55:47" itemprop="dateModified" datetime="2022-01-16T10:55:47+08:00">2022-01-16</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yogattt"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">yogattt</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yogattt</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<!-- 樱花特效 -->
  
</body>
</html>
